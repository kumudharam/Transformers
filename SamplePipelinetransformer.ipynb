{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SamplePipelinetransformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlbe89+lBOhM4qdUS2I170",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumudharam/Transformers/blob/main/SamplePipelinetransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "o_xrRa_8DIJD",
        "outputId": "617c8493-1b29-4d2c-b507-f4d906b76a30"
      },
      "source": [
        "from collections.abc import Iterable\n",
        "from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from ..data import SquadExample, SquadFeatures, squad_convert_examples_to_features\n",
        "from ..file_utils import PaddingStrategy, add_end_docstrings, is_tf_available, is_torch_available\n",
        "from ..modelcard import ModelCard\n",
        "from ..tokenization_utils import PreTrainedTokenizer\n",
        "from .base import PIPELINE_INIT_ARGS, ArgumentHandler, Pipeline\n",
        "\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from ..modeling_tf_utils import TFPreTrainedModel\n",
        "    from ..modeling_utils import PreTrainedModel\n",
        "\n",
        "if is_tf_available():\n",
        "    import tensorflow as tf\n",
        "\n",
        "    from ..models.auto.modeling_tf_auto import TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
        "\n",
        "if is_torch_available():\n",
        "    import torch\n",
        "\n",
        "    from ..models.auto.modeling_auto import MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class QuestionAnsweringArgumentHandler(ArgumentHandler):\n",
        "    \"\"\"\n",
        "    QuestionAnsweringPipeline requires the user to provide multiple arguments (i.e. question & context) to be mapped to\n",
        "    internal :class:`~transformers.SquadExample`.\n",
        "\n",
        "    QuestionAnsweringArgumentHandler manages all the possible to create a :class:`~transformers.SquadExample` from the\n",
        "    command-line supplied arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    def normalize(self, item):\n",
        "        if isinstance(item, SquadExample):\n",
        "            return item\n",
        "        elif isinstance(item, dict):\n",
        "            for k in [\"question\", \"context\"]:\n",
        "                if k not in item:\n",
        "                    raise KeyError(\"You need to provide a dictionary with keys {question:..., context:...}\")\n",
        "                elif item[k] is None:\n",
        "                    raise ValueError(f\"`{k}` cannot be None\")\n",
        "                elif isinstance(item[k], str) and len(item[k]) == 0:\n",
        "                    raise ValueError(f\"`{k}` cannot be empty\")\n",
        "\n",
        "            return QuestionAnsweringPipeline.create_sample(**item)\n",
        "        raise ValueError(f\"{item} argument needs to be of type (SquadExample, dict)\")\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        # Detect where the actual inputs are\n",
        "        if args is not None and len(args) > 0:\n",
        "            if len(args) == 1:\n",
        "                inputs = args[0]\n",
        "            elif len(args) == 2 and {type(el) for el in args} == {str}:\n",
        "                inputs = [{\"question\": args[0], \"context\": args[1]}]\n",
        "            else:\n",
        "                inputs = list(args)\n",
        "        # Generic compatibility with sklearn and Keras\n",
        "        # Batched data\n",
        "        elif \"X\" in kwargs:\n",
        "            inputs = kwargs[\"X\"]\n",
        "        elif \"data\" in kwargs:\n",
        "            inputs = kwargs[\"data\"]\n",
        "        elif \"question\" in kwargs and \"context\" in kwargs:\n",
        "            if isinstance(kwargs[\"question\"], list) and isinstance(kwargs[\"context\"], str):\n",
        "                inputs = [{\"question\": Q, \"context\": kwargs[\"context\"]} for Q in kwargs[\"question\"]]\n",
        "            elif isinstance(kwargs[\"question\"], list) and isinstance(kwargs[\"context\"], list):\n",
        "                if len(kwargs[\"question\"]) != len(kwargs[\"context\"]):\n",
        "                    raise ValueError(\"Questions and contexts don't have the same lengths\")\n",
        "\n",
        "                inputs = [{\"question\": Q, \"context\": C} for Q, C in zip(kwargs[\"question\"], kwargs[\"context\"])]\n",
        "            elif isinstance(kwargs[\"question\"], str) and isinstance(kwargs[\"context\"], str):\n",
        "                inputs = [{\"question\": kwargs[\"question\"], \"context\": kwargs[\"context\"]}]\n",
        "            else:\n",
        "                raise ValueError(\"Arguments can't be understood\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown arguments {kwargs}\")\n",
        "\n",
        "        # Normalize inputs\n",
        "        if isinstance(inputs, dict):\n",
        "            inputs = [inputs]\n",
        "        elif isinstance(inputs, Iterable):\n",
        "            # Copy to avoid overriding arguments\n",
        "            inputs = [i for i in inputs]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid arguments {kwargs}\")\n",
        "\n",
        "        for i, item in enumerate(inputs):\n",
        "            inputs[i] = self.normalize(item)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@add_end_docstrings(PIPELINE_INIT_ARGS)\n",
        "class QuestionAnsweringPipeline(Pipeline):\n",
        "    \"\"\"\n",
        "    Question Answering pipeline using any :obj:`ModelForQuestionAnswering`. See the `question answering examples\n",
        "    <../task_summary.html#question-answering>`__ for more information.\n",
        "\n",
        "    This question answering pipeline can currently be loaded from :func:`~transformers.pipeline` using the following\n",
        "    task identifier: :obj:`\"question-answering\"`.\n",
        "\n",
        "    The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\n",
        "    up-to-date list of available models on `huggingface.co/models\n",
        "    <https://huggingface.co/models?filter=question-answering>`__.\n",
        "    \"\"\"\n",
        "\n",
        "    default_input_names = \"question,context\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Union[\"PreTrainedModel\", \"TFPreTrainedModel\"],\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        modelcard: Optional[ModelCard] = None,\n",
        "        framework: Optional[str] = None,\n",
        "        device: int = -1,\n",
        "        task: str = \"\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            modelcard=modelcard,\n",
        "            framework=framework,\n",
        "            device=device,\n",
        "            task=task,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self._args_parser = QuestionAnsweringArgumentHandler()\n",
        "        self.check_model_type(\n",
        "            TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING if self.framework == \"tf\" else MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def create_sample(\n",
        "        question: Union[str, List[str]], context: Union[str, List[str]]\n",
        "    ) -> Union[SquadExample, List[SquadExample]]:\n",
        "        \"\"\"\n",
        "        QuestionAnsweringPipeline leverages the :class:`~transformers.SquadExample` internally. This helper method\n",
        "        encapsulate all the logic for converting question(s) and context(s) to :class:`~transformers.SquadExample`.\n",
        "\n",
        "        We currently support extractive question answering.\n",
        "\n",
        "        Arguments:\n",
        "            question (:obj:`str` or :obj:`List[str]`): The question(s) asked.\n",
        "            context (:obj:`str` or :obj:`List[str]`): The context(s) in which we will look for the answer.\n",
        "\n",
        "        Returns:\n",
        "            One or a list of :class:`~transformers.SquadExample`: The corresponding :class:`~transformers.SquadExample`\n",
        "            grouping question and context.\n",
        "        \"\"\"\n",
        "        if isinstance(question, list):\n",
        "            return [SquadExample(None, q, c, None, None, None) for q, c in zip(question, context)]\n",
        "        else:\n",
        "            return SquadExample(None, question, context, None, None, None)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Answer the question(s) given as inputs by using the context(s).\n",
        "\n",
        "        Args:\n",
        "            args (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`):\n",
        "                One or several :class:`~transformers.SquadExample` containing the question and context.\n",
        "            X (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`, `optional`):\n",
        "                One or several :class:`~transformers.SquadExample` containing the question and context (will be treated\n",
        "                the same way as if passed as the first positional argument).\n",
        "            data (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`, `optional`):\n",
        "                One or several :class:`~transformers.SquadExample` containing the question and context (will be treated\n",
        "                the same way as if passed as the first positional argument).\n",
        "            question (:obj:`str` or :obj:`List[str]`):\n",
        "                One or several question(s) (must be used in conjunction with the :obj:`context` argument).\n",
        "            context (:obj:`str` or :obj:`List[str]`):\n",
        "                One or several context(s) associated with the question(s) (must be used in conjunction with the\n",
        "                :obj:`question` argument).\n",
        "            topk (:obj:`int`, `optional`, defaults to 1):\n",
        "                The number of answers to return (will be chosen by order of likelihood). Note that we return less than\n",
        "                topk answers if there are not enough options available within the context.\n",
        "            doc_stride (:obj:`int`, `optional`, defaults to 128):\n",
        "                If the context is too long to fit with the question for the model, it will be split in several chunks\n",
        "                with some overlap. This argument controls the size of that overlap.\n",
        "            max_answer_len (:obj:`int`, `optional`, defaults to 15):\n",
        "                The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n",
        "            max_seq_len (:obj:`int`, `optional`, defaults to 384):\n",
        "                The maximum length of the total sentence (context + question) after tokenization. The context will be\n",
        "                split in several chunks (using :obj:`doc_stride`) if needed.\n",
        "            max_question_len (:obj:`int`, `optional`, defaults to 64):\n",
        "                The maximum length of the question after tokenization. It will be truncated if needed.\n",
        "            handle_impossible_answer (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "                Whether or not we accept impossible as an answer.\n",
        "\n",
        "        Return:\n",
        "            A :obj:`dict` or a list of :obj:`dict`: Each result comes as a dictionary with the following keys:\n",
        "\n",
        "            - **score** (:obj:`float`) -- The probability associated to the answer.\n",
        "            - **start** (:obj:`int`) -- The character start index of the answer (in the tokenized version of the\n",
        "              input).\n",
        "            - **end** (:obj:`int`) -- The character end index of the answer (in the tokenized version of the input).\n",
        "            - **answer** (:obj:`str`) -- The answer to the question.\n",
        "        \"\"\"\n",
        "        # Set defaults values\n",
        "        kwargs.setdefault(\"padding\", \"longest\")\n",
        "        kwargs.setdefault(\"topk\", 1)\n",
        "        kwargs.setdefault(\"doc_stride\", 128)\n",
        "        kwargs.setdefault(\"max_answer_len\", 15)\n",
        "        kwargs.setdefault(\"max_seq_len\", 384)\n",
        "        kwargs.setdefault(\"max_question_len\", 64)\n",
        "        kwargs.setdefault(\"handle_impossible_answer\", False)\n",
        "\n",
        "        if kwargs[\"topk\"] < 1:\n",
        "            raise ValueError(f\"topk parameter should be >= 1 (got {kwargs['topk']})\")\n",
        "\n",
        "        if kwargs[\"max_answer_len\"] < 1:\n",
        "            raise ValueError(f\"max_answer_len parameter should be >= 1 (got {(kwargs['max_answer_len'])}\")\n",
        "\n",
        "        # Convert inputs to features\n",
        "        examples = self._args_parser(*args, **kwargs)\n",
        "        if not self.tokenizer.is_fast:\n",
        "            features_list = [\n",
        "                squad_convert_examples_to_features(\n",
        "                    examples=[example],\n",
        "                    tokenizer=self.tokenizer,\n",
        "                    max_seq_length=kwargs[\"max_seq_len\"],\n",
        "                    doc_stride=kwargs[\"doc_stride\"],\n",
        "                    max_query_length=kwargs[\"max_question_len\"],\n",
        "                    padding_strategy=PaddingStrategy.MAX_LENGTH.value,\n",
        "                    is_training=False,\n",
        "                    tqdm_enabled=False,\n",
        "                )\n",
        "                for example in examples\n",
        "            ]\n",
        "        else:\n",
        "            features_list = []\n",
        "            for example in examples:\n",
        "                # Define the side we want to truncate / pad and the text/pair sorting\n",
        "                question_first = bool(self.tokenizer.padding_side == \"right\")\n",
        "\n",
        "                encoded_inputs = self.tokenizer(\n",
        "                    text=example.question_text if question_first else example.context_text,\n",
        "                    text_pair=example.context_text if question_first else example.question_text,\n",
        "                    padding=kwargs[\"padding\"],\n",
        "                    truncation=\"only_second\" if question_first else \"only_first\",\n",
        "                    max_length=kwargs[\"max_seq_len\"],\n",
        "                    stride=kwargs[\"doc_stride\"],\n",
        "                    return_tensors=\"np\",\n",
        "                    return_token_type_ids=True,\n",
        "                    return_overflowing_tokens=True,\n",
        "                    return_offsets_mapping=True,\n",
        "                    return_special_tokens_mask=True,\n",
        "                )\n",
        "\n",
        "                # When the input is too long, it's converted in a batch of inputs with overflowing tokens\n",
        "                # and a stride of overlap between the inputs. If a batch of inputs is given, a special output\n",
        "                # \"overflow_to_sample_mapping\" indicate which member of the encoded batch belong to which original batch sample.\n",
        "                # Here we tokenize examples one-by-one so we don't need to use \"overflow_to_sample_mapping\".\n",
        "                # \"num_span\" is the number of output samples generated from the overflowing tokens.\n",
        "                num_spans = len(encoded_inputs[\"input_ids\"])\n",
        "\n",
        "                # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
        "                # We put 0 on the tokens from the context and 1 everywhere else (question and special tokens)\n",
        "                p_mask = np.asarray(\n",
        "                    [\n",
        "                        [tok != 1 if question_first else 0 for tok in encoded_inputs.sequence_ids(span_id)]\n",
        "                        for span_id in range(num_spans)\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                # keep the cls_token unmasked (some models use it to indicate unanswerable questions)\n",
        "                if self.tokenizer.cls_token_id is not None:\n",
        "                    cls_index = np.nonzero(encoded_inputs[\"input_ids\"] == self.tokenizer.cls_token_id)\n",
        "                    p_mask[cls_index] = 0\n",
        "\n",
        "                features = []\n",
        "                for span_idx in range(num_spans):\n",
        "                    features.append(\n",
        "                        SquadFeatures(\n",
        "                            input_ids=encoded_inputs[\"input_ids\"][span_idx],\n",
        "                            attention_mask=encoded_inputs[\"attention_mask\"][span_idx],\n",
        "                            token_type_ids=encoded_inputs[\"token_type_ids\"][span_idx],\n",
        "                            p_mask=p_mask[span_idx].tolist(),\n",
        "                            encoding=encoded_inputs[span_idx],\n",
        "                            # We don't use the rest of the values - and actually\n",
        "                            # for Fast tokenizer we could totally avoid using SquadFeatures and SquadExample\n",
        "                            cls_index=None,\n",
        "                            token_to_orig_map={},\n",
        "                            example_index=0,\n",
        "                            unique_id=0,\n",
        "                            paragraph_len=0,\n",
        "                            token_is_max_context=0,\n",
        "                            tokens=[],\n",
        "                            start_position=0,\n",
        "                            end_position=0,\n",
        "                            is_impossible=False,\n",
        "                            qas_id=None,\n",
        "                        )\n",
        "                    )\n",
        "                features_list.append(features)\n",
        "\n",
        "        all_answers = []\n",
        "        for features, example in zip(features_list, examples):\n",
        "            model_input_names = self.tokenizer.model_input_names\n",
        "            fw_args = {k: [feature.__dict__[k] for feature in features] for k in model_input_names}\n",
        "\n",
        "            # Manage tensor allocation on correct device\n",
        "            with self.device_placement():\n",
        "                if self.framework == \"tf\":\n",
        "                    fw_args = {k: tf.constant(v) for (k, v) in fw_args.items()}\n",
        "                    start, end = self.model(fw_args)[:2]\n",
        "                    start, end = start.numpy(), end.numpy()\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        # Retrieve the score for the context tokens only (removing question tokens)\n",
        "                        fw_args = {k: torch.tensor(v, device=self.device) for (k, v) in fw_args.items()}\n",
        "                        # On Windows, the default int type in numpy is np.int32 so we get some non-long tensors.\n",
        "                        fw_args = {k: v.long() if v.dtype == torch.int32 else v for (k, v) in fw_args.items()}\n",
        "                        start, end = self.model(**fw_args)[:2]\n",
        "                        start, end = start.cpu().numpy(), end.cpu().numpy()\n",
        "\n",
        "            min_null_score = 1000000  # large and positive\n",
        "            answers = []\n",
        "            for (feature, start_, end_) in zip(features, start, end):\n",
        "                # Ensure padded tokens & question tokens cannot belong to the set of candidate answers.\n",
        "                undesired_tokens = np.abs(np.array(feature.p_mask) - 1) & feature.attention_mask\n",
        "\n",
        "                # Generate mask\n",
        "                undesired_tokens_mask = undesired_tokens == 0.0\n",
        "\n",
        "                # Make sure non-context indexes in the tensor cannot contribute to the softmax\n",
        "                start_ = np.where(undesired_tokens_mask, -10000.0, start_)\n",
        "                end_ = np.where(undesired_tokens_mask, -10000.0, end_)\n",
        "\n",
        "                # Normalize logits and spans to retrieve the answer\n",
        "                start_ = np.exp(start_ - np.log(np.sum(np.exp(start_), axis=-1, keepdims=True)))\n",
        "                end_ = np.exp(end_ - np.log(np.sum(np.exp(end_), axis=-1, keepdims=True)))\n",
        "\n",
        "                if kwargs[\"handle_impossible_answer\"]:\n",
        "                    min_null_score = min(min_null_score, (start_[0] * end_[0]).item())\n",
        "\n",
        "                # Mask CLS\n",
        "                start_[0] = end_[0] = 0.0\n",
        "\n",
        "                starts, ends, scores = self.decode(\n",
        "                    start_, end_, kwargs[\"topk\"], kwargs[\"max_answer_len\"], undesired_tokens\n",
        "                )\n",
        "                if not self.tokenizer.is_fast:\n",
        "                    char_to_word = np.array(example.char_to_word_offset)\n",
        "\n",
        "                    # Convert the answer (tokens) back to the original text\n",
        "                    # Score: score from the model\n",
        "                    # Start: Index of the first character of the answer in the context string\n",
        "                    # End: Index of the character following the last character of the answer in the context string\n",
        "                    # Answer: Plain text of the answer\n",
        "                    answers += [\n",
        "                        {\n",
        "                            \"score\": score.item(),\n",
        "                            \"start\": np.where(char_to_word == feature.token_to_orig_map[s])[0][0].item(),\n",
        "                            \"end\": np.where(char_to_word == feature.token_to_orig_map[e])[0][-1].item(),\n",
        "                            \"answer\": \" \".join(\n",
        "                                example.doc_tokens[feature.token_to_orig_map[s] : feature.token_to_orig_map[e] + 1]\n",
        "                            ),\n",
        "                        }\n",
        "                        for s, e, score in zip(starts, ends, scores)\n",
        "                    ]\n",
        "                else:\n",
        "                    # Convert the answer (tokens) back to the original text\n",
        "                    # Score: score from the model\n",
        "                    # Start: Index of the first character of the answer in the context string\n",
        "                    # End: Index of the character following the last character of the answer in the context string\n",
        "                    # Answer: Plain text of the answer\n",
        "                    question_first = bool(self.tokenizer.padding_side == \"right\")\n",
        "                    enc = feature.encoding\n",
        "\n",
        "                    # Sometimes the max probability token is in the middle of a word so:\n",
        "                    # - we start by finding the right word containing the token with `token_to_word`\n",
        "                    # - then we convert this word in a character span with `word_to_chars`\n",
        "                    answers += [\n",
        "                        {\n",
        "                            \"score\": score.item(),\n",
        "                            \"start\": enc.word_to_chars(\n",
        "                                enc.token_to_word(s), sequence_index=1 if question_first else 0\n",
        "                            )[0],\n",
        "                            \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1 if question_first else 0)[\n",
        "                                1\n",
        "                            ],\n",
        "                            \"answer\": example.context_text[\n",
        "                                enc.word_to_chars(enc.token_to_word(s), sequence_index=1 if question_first else 0)[\n",
        "                                    0\n",
        "                                ] : enc.word_to_chars(enc.token_to_word(e), sequence_index=1 if question_first else 0)[\n",
        "                                    1\n",
        "                                ]\n",
        "                            ],\n",
        "                        }\n",
        "                        for s, e, score in zip(starts, ends, scores)\n",
        "                    ]\n",
        "\n",
        "            if kwargs[\"handle_impossible_answer\"]:\n",
        "                answers.append({\"score\": min_null_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
        "\n",
        "            answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: kwargs[\"topk\"]]\n",
        "            all_answers += answers\n",
        "\n",
        "        if len(all_answers) == 1:\n",
        "            return all_answers[0]\n",
        "        return all_answers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def decode(\n",
        "        self, start: np.ndarray, end: np.ndarray, topk: int, max_answer_len: int, undesired_tokens: np.ndarray\n",
        "    ) -> Tuple:\n",
        "        \"\"\"\n",
        "        Take the output of any :obj:`ModelForQuestionAnswering` and will generate probabilities for each span to be the\n",
        "        actual answer.\n",
        "\n",
        "        In addition, it filters out some unwanted/impossible cases like answer len being greater than max_answer_len or\n",
        "        answer end position being before the starting position. The method supports output the k-best answer through\n",
        "        the topk argument.\n",
        "\n",
        "        Args:\n",
        "            start (:obj:`np.ndarray`): Individual start probabilities for each token.\n",
        "            end (:obj:`np.ndarray`): Individual end probabilities for each token.\n",
        "            topk (:obj:`int`): Indicates how many possible answer span(s) to extract from the model output.\n",
        "            max_answer_len (:obj:`int`): Maximum size of the answer to extract from the model's output.\n",
        "            undesired_tokens (:obj:`np.ndarray`): Mask determining tokens that can be part of the answer\n",
        "        \"\"\"\n",
        "        # Ensure we have batch axis\n",
        "        if start.ndim == 1:\n",
        "            start = start[None]\n",
        "\n",
        "        if end.ndim == 1:\n",
        "            end = end[None]\n",
        "\n",
        "        # Compute the score of each tuple(start, end) to be the real answer\n",
        "        outer = np.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))\n",
        "\n",
        "        # Remove candidate with end < start and end - start > max_answer_len\n",
        "        candidates = np.tril(np.triu(outer), max_answer_len - 1)\n",
        "\n",
        "        #  Inspired by Chen & al. (https://github.com/facebookresearch/DrQA)\n",
        "        scores_flat = candidates.flatten()\n",
        "        if topk == 1:\n",
        "            idx_sort = [np.argmax(scores_flat)]\n",
        "        elif len(scores_flat) < topk:\n",
        "            idx_sort = np.argsort(-scores_flat)\n",
        "        else:\n",
        "            idx = np.argpartition(-scores_flat, topk)[0:topk]\n",
        "            idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
        "\n",
        "        starts, ends = np.unravel_index(idx_sort, candidates.shape)[1:]\n",
        "        desired_spans = np.isin(starts, undesired_tokens.nonzero()) & np.isin(ends, undesired_tokens.nonzero())\n",
        "        starts = starts[desired_spans]\n",
        "        ends = ends[desired_spans]\n",
        "        scores = candidates[0, starts, ends]\n",
        "\n",
        "        return starts, ends, scores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def span_to_answer(self, text: str, start: int, end: int) -> Dict[str, Union[str, int]]:\n",
        "        \"\"\"\n",
        "        When decoding from token probabilities, this method maps token indexes to actual word in the initial context.\n",
        "\n",
        "        Args:\n",
        "            text (:obj:`str`): The actual context to extract the answer from.\n",
        "            start (:obj:`int`): The answer starting token index.\n",
        "            end (:obj:`int`): The answer end token index.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary like :obj:`{'answer': str, 'start': int, 'end': int}`\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        token_idx = char_start_idx = char_end_idx = chars_idx = 0\n",
        "\n",
        "        for i, word in enumerate(text.split(\" \")):\n",
        "            token = self.tokenizer.tokenize(word)\n",
        "\n",
        "            # Append words if they are in the span\n",
        "            if start <= token_idx <= end:\n",
        "                if token_idx == start:\n",
        "                    char_start_idx = chars_idx\n",
        "\n",
        "                if token_idx == end:\n",
        "                    char_end_idx = chars_idx + len(word)\n",
        "\n",
        "                words += [word]\n",
        "\n",
        "            # Stop if we went over the end of the answer\n",
        "            if token_idx > end:\n",
        "                break\n",
        "\n",
        "            # Append the subtokenization length to the running index\n",
        "            token_idx += len(token)\n",
        "            chars_idx += len(word) + 1\n",
        "\n",
        "        # Join text with spaces\n",
        "        return {\n",
        "            \"answer\": \" \".join(words),\n",
        "            \"start\": max(0, char_start_idx),\n",
        "            \"end\": min(len(text), char_end_idx),\n",
        "        }\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-92a179d3583a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSquadExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSquadFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquad_convert_examples_to_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_end_docstrings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelcard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}