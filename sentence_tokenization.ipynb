{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence-tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumudharam/Transformers/blob/main/sentence_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki06xBcRSuQf",
        "outputId": "95f2f509-a882-464b-b0ed-731151eb8c63"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install timeline \n",
        "!pip install pipeline\n",
        "!pip install pipelines \n",
        "!pip install openpyxl \n",
        "!pip install xlrd\n",
        "!pip install xlsxwriter "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Collecting timeline\n",
            "  Downloading timeline-0.0.7.tar.gz (11 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from timeline) (2018.9)\n",
            "Building wheels for collected packages: timeline\n",
            "  Building wheel for timeline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeline: filename=timeline-0.0.7-py3-none-any.whl size=17005 sha256=101993f693783ad053d6ca683600730b16e32bd93ab23bf3b4469e6e57e95492\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/03/7d/af6f2e055c7a95e6e9dcd5e64a9738678cac6bd8b05d4e1bd3\n",
            "Successfully built timeline\n",
            "Installing collected packages: timeline\n",
            "Successfully installed timeline-0.0.7\n",
            "Collecting pipeline\n",
            "  Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n",
            "Installing collected packages: pipeline\n",
            "Successfully installed pipeline-0.1.0\n",
            "Collecting pipelines\n",
            "  Downloading pipelines-0.0.14.tar.gz (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting futures==3.0.5\n",
            "  Downloading futures-3.0.5.tar.gz (25 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/55/db/97c1ca37edab586a1ae03d6892b6633d8eaa23b23ac40c7e5bbc55423c78/futures-3.0.5.tar.gz#sha256=0542525145d5afc984c88f914a0c85c77527f65946617edb5274f72406f981df (from https://pypi.org/simple/futures/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "Collecting pipelines\n",
            "  Downloading pipelines-0.0.12.tar.gz (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 53.1 MB/s \n",
            "\u001b[?25h  Downloading pipelines-0.0.11.tar.gz (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 38.2 MB/s \n",
            "\u001b[?25h  Downloading pipelines-0.0.10.tar.gz (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 49.9 MB/s \n",
            "\u001b[?25h  Downloading pipelines-0.0.9.tar.gz (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 51.1 MB/s \n",
            "\u001b[?25h  Downloading pipelines-0.0.4.tar.gz (10 kB)\n",
            "  Downloading pipelines-0.0.3.tar.gz (9.6 kB)\n",
            "  Downloading pipelines-0.0.1.tar.gz (8.7 kB)\n",
            "Building wheels for collected packages: pipelines\n",
            "  Building wheel for pipelines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pipelines: filename=pipelines-0.0.1-py3-none-any.whl size=15784 sha256=592acbaa1d2d3e29c61b0043c72400e56ecf1086be77e3de65e77cfeb8125974\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/8a/50/564066c923abb3bfbafe1cde0d19607085ab4c66f572c1b169\n",
            "Successfully built pipelines\n",
            "Installing collected packages: pipelines\n",
            "Successfully installed pipelines-0.0.1\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.1-py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 8.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGchc1tMDfCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479d5861-3a8f-40bf-a891-c8162c0acdc5"
      },
      "source": [
        "!pip install pandas nltk spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.2.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VCOMiHrEqRQ",
        "outputId": "24a6823b-dd12-476b-e6bd-580455e73c52"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLovth7afRFr"
      },
      "source": [
        "import os\n",
        "\n",
        "# On Jupyter Notebook or Colab\n",
        "DIR_PATH = os.getcwd()\n",
        "# On Python module\n",
        "#DIR_PATH = os.path.dirname(__file__)\n",
        "\n",
        "FILE_PATH = os.path.join(DIR_PATH, \"scripts.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJJrXlJID1bg",
        "outputId": "f02386a3-a6a3-4eb4-e6a5-e06fe09fa2a5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the file\n",
        "df = pd.read_csv(r'/content/TestData.csv')\n",
        "# Remove NaN values\n",
        "df = df[~df[\"Dialogue\"].isna()]\n",
        "# Assign first_dialogue to the first row's \"Dialogue\" column\n",
        "first_dialogue = df.loc[0, \"Dialogue\"]\n",
        "print(first_dialogue)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GIVEN that I'm on any Screen of the application.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HmqACyrSD5-f",
        "outputId": "19500b92-7cdd-44ff-a684-0fbe60d8d5bf"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dialogue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GIVEN that I'm on any Screen of the application.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WHEN I fill the created before as like other f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AND click the Apply filters to get the filtere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THEN the application will display the data bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IF I close the application and Launch again.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Dialogue\n",
              "0   GIVEN that I'm on any Screen of the application.\n",
              "1  WHEN I fill the created before as like other f...\n",
              "2  AND click the Apply filters to get the filtere...\n",
              "3  THEN the application will display the data bas...\n",
              "4      IF I close the application and Launch again. "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkD7e6AdEEQ-",
        "outputId": "abf3e554-e4a2-48e3-d683-aaecddf3bf80"
      },
      "source": [
        "# use Python's split\n",
        "first_dialogue.split(\".\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application\", '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipRRtATkEW7l",
        "outputId": "d9ab1197-0526-4707-8b60-d9c801177353"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_tokenize(first_dialogue)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7PqnpEygfdw",
        "outputId": "b28f9a0d-0ee5-4c55-fbf3-e8b2c82ad0a3"
      },
      "source": [
        "%%timeit -n 10\n",
        "df.loc[:5000, \"Dialogue\"].apply(lambda x: sent_tokenize(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 584 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx5WECM2FQIb",
        "outputId": "1e05ea85-d8ae-4309-9bd0-b7c811535ea7"
      },
      "source": [
        "import spacy\n",
        "# use spacy with the dependency parse \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "[str(sent) for sent in nlp(first_dialogue).sents]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWuGSjNdMJI7"
      },
      "source": [
        "#%%timeit -n 10\n",
        "# WARNING: takes a long time!\n",
        "#df.loc[:5000, \"Dialogue\"].apply(lambda x: [sent.text for sent in nlp(x).sents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToodwNUzLkBZ",
        "outputId": "5a2bb507-4fc1-4b62-97db-1ac9ec5c012a"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "# use spacy with the sentencizer\n",
        "nlp = English()  # just the language with no model\n",
        "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
        "nlp.add_pipe(sentencizer)\n",
        "[str(sent) for sent in nlp(first_dialogue).sents]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pXpftxcnlfz",
        "outputId": "84c797b9-40ac-46ce-a03d-4624f97b6cd9"
      },
      "source": [
        "%%timeit -n 10\n",
        "df.loc[:5000, \"Dialogue\"].apply(lambda x: [sent.text for sent in nlp(x).sents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 758 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvYfpERJIRKT",
        "outputId": "e0eeb085-38b8-4e0d-e909-2da492bda5d8"
      },
      "source": [
        "import re\n",
        "# use regular expression\n",
        "rule = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
        "re.split(rule, first_dialogue)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfSscvTri0Wk",
        "outputId": "a8e481b4-9a26-413e-8c3c-333cf7f825bd"
      },
      "source": [
        "%%timeit -n 10\n",
        "df.loc[:5000, \"Dialogue\"].apply(lambda x: re.split(rule, x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 361 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxkl0c-YjGPI",
        "outputId": "d68083df-4ec7-4457-eab6-28f2e7d614c7"
      },
      "source": [
        "%%timeit -n 10\n",
        "# Without dependency parser\n",
        "tokenized_data = []\n",
        "first_5000_rows = df.loc[:5000, \"Dialogue\"]\n",
        "for doc in nlp.pipe(first_5000_rows, batch_size=20):\n",
        "  tokenized_data.append([sent.text for sent in doc.sents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 470 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xQf3YpHhGnj"
      },
      "source": [
        "# Transform data using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# WARNING: takes a long time!\n",
        "df[\"Dialogue\"] = df[\"Dialogue\"].apply(lambda x: [sent.text for sent in nlp(x).sents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcgRauYoiL9L"
      },
      "source": [
        "df = df.explode(\"Dialogue\", ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wgo87wAzR20"
      },
      "source": [
        "df.rename(columns={\"Unnamed: 0\": \"Dialogue ID\"}, inplace=True)\n",
        "df.index.name = \"Sentence ID\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "5T3ooPLszySR",
        "outputId": "fca1b7ad-78ad-49e3-a078-6e1ae43805c2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dialogue</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sentence ID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GIVEN that I'm on any Screen of the application.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WHEN I fill the created before as like other f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AND click the Apply filters to get the filtere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THEN the application will display the data bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IF I close the application and Launch again.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Dialogue\n",
              "Sentence ID                                                   \n",
              "0             GIVEN that I'm on any Screen of the application.\n",
              "1            WHEN I fill the created before as like other f...\n",
              "2            AND click the Apply filters to get the filtere...\n",
              "3            THEN the application will display the data bas...\n",
              "4                 IF I close the application and Launch again."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsTZ2qZwz2MX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f7276d-156e-45ca-c4ee-e55ec605e545"
      },
      "source": [
        "Text = df.to_csv(\"scripts_tokenized.csv\")\n",
        "print(df.to_csv(\"scripts_tokenized.csv\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO-XEEhSKeGv",
        "outputId": "004a07dc-5ad7-4c8b-8b33-4288898d0913"
      },
      "source": [
        "!pip install -U transformers==3.0.0 --quiet\n",
        "!python -m nltk.downloader punkt\n",
        "\n",
        "!git clone https://github.com/patil-suraj/question_generation.git\n",
        "%cd question_generation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from transformers import pipeline\n",
        "import timeline\n",
        "import pandas as pd\n",
        "import csv \n",
        "import openpyxl\n",
        "import xlsxwriter\n",
        "from itertools import chain \n",
        "workbook = xlsxwriter.Workbook('/content/TestData1.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "\n",
        "# Read the file\n",
        "df1 = pd.read_csv(r'/content/TestData.csv')\n",
        "df = pd.DataFrame(df1, columns =['AccpCriteria'])\n",
        " # Iterate over the sequence of column names\n",
        "for column in reversed(df.columns):\n",
        "    \n",
        "    # Select column contents by column\n",
        "    # name using [] operator\n",
        "    columnSeriesObj = df[column]\n",
        "    print('Column Name : ', column)\n",
        "    print('Column Contents : ', columnSeriesObj.values)\n",
        "\n",
        "    Stringval = str(columnSeriesObj.values)\n",
        "    print(Stringval)\n",
        "\n",
        "from pipelines import pipeline\n",
        "nlp = pipeline(\"question-generation\") \n",
        "\n",
        "nlp(Stringval)\n",
        "\n",
        "\n",
        "# Driver code\n",
        "word = nlp(Stringval)\n",
        "#word1 = split(word)\n",
        "#returns only values\n",
        "\n",
        "values = []\n",
        "for dictionary in word:\n",
        "     values.extend([v for k, v in dictionary.items() if k == 'answer'])\n",
        "print(values)\n",
        "\n",
        "def split(word):\n",
        "    return [char for char in word]\n",
        "word2 = split(values)\n",
        "header1 = \"Steps\"\n",
        "header2 = \"Expected_result\"\n",
        "worksheet.write('A1', header1)\n",
        "worksheet.write('B1', header2)\n",
        "\n",
        "row = 1\n",
        "column = 1\n",
        "content = word2\n",
        "  \n",
        "# iterating through content list\n",
        "for item in content :\n",
        "    worksheet.write(row, column, item)\n",
        "    row += 1\n",
        "\n",
        "values1 = []\n",
        "for dictionary in word:\n",
        "     values1.extend([v for k, v in dictionary.items() if k == 'question'])\n",
        "word2 = split(values1)\n",
        "\n",
        "n = [\"happens\",\"do\", \"will\", \"is\", \"are\", \"if\", \"?\", \"[\", \"]\", \"``\", \"'\", \"'\", \"[ \"]     \n",
        "stopwords = set(n)\n",
        "word_tokens = word_tokenize(str(values1))\n",
        "\n",
        "new_strings = [w for w in word_tokens if not w.lower() in stopwords]\n",
        "\n",
        "new_strings = []\n",
        "for string in word_tokens:\n",
        "  if string not in stopwords:\n",
        "     \n",
        "     #Prefix = \"Check\" \n",
        "     new_string = string.replace(\"What\", \"Check whether\")\n",
        "     new_strings.append(new_string)\n",
        "     #listToStr = split(new_strings)\n",
        "     listToStr = ' '.join(new_strings)\n",
        "     \n",
        "     listToStr1 = listToStr.split(', ')\n",
        "\n",
        "\n",
        "print(listToStr1)\n",
        "\n",
        "row = 1\n",
        "column = 0\n",
        "word3 = listToStr1\n",
        "\n",
        "# iterating through content list\n",
        "for item in word3:\n",
        "    worksheet.write_string(row, column, item)\n",
        "    row += 1\n",
        "workbook.close()   \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Cloning into 'question_generation'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 265 (delta 100), reused 95 (delta 95), pack-reused 156\u001b[K\n",
            "Receiving objects: 100% (265/265), 289.06 KiB | 17.00 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n",
            "/content/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Column Name :  Dialogue\n",
            "Column Contents :  [\"GIVEN that I'm on any Screen of the application.\"\n",
            " 'WHEN I fill the created before as like other filters.'\n",
            " 'AND click the Apply filters to get the filtered value from the backend.'\n",
            " 'THEN the application will display the data based on the applied filters. '\n",
            " 'IF I close the application and Launch again. '\n",
            " 'THEN the recently chosen filters including created before will be applied automatically from the local storage.'\n",
            " 'THEN display the corresponding last access page.']\n",
            "[\"GIVEN that I'm on any Screen of the application.\"\n",
            " 'WHEN I fill the created before as like other filters.'\n",
            " 'AND click the Apply filters to get the filtered value from the backend.'\n",
            " 'THEN the application will display the data based on the applied filters. '\n",
            " 'IF I close the application and Launch again. '\n",
            " 'THEN the recently chosen filters including created before will be applied automatically from the local storage.'\n",
            " 'THEN display the corresponding last access page.']\n",
            "['any Screen of the application', 'fill the created before as like other filters', 'Apply filters', 'the application will display the data based on the applied filters', 'Launch again', 'created before', 'last access page']\n",
            "[\"Check whether the name of the screen that I 'm on \", \"Check whether I when I 'm on any Screen of the application \", \"'Check whether get the filtered value from the backend \", \"'Check whether the application display the data based on \", \"'Check whether I close the application \", \"'Check whether be applied automatically from the local storage \", \"Check whether page does 'THEN display ''\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjjwfdEgWUdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed263fd1-1b24-41dc-86f3-afce55daf12a"
      },
      "source": [
        " !pip install -U transformers==3.0.0 --quiet\n",
        "!python -m nltk.downloader punkt\n",
        "\n",
        "!git clone https://github.com/patil-suraj/question_generation.git\n",
        "%cd question_generation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from transformers import pipeline\n",
        "import timeline\n",
        "import pandas as pd\n",
        "import csv \n",
        "import openpyxl\n",
        "import xlsxwriter\n",
        "from itertools import chain \n",
        "workbook = xlsxwriter.Workbook('/content/TestCaseOutput.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "\n",
        "# Read the file\n",
        "df1 = pd.read_csv(r'/content/TestScenario8.csv')\n",
        "df = pd.DataFrame(df1, columns =['AccpCriteria'])\n",
        " # Iterate over the sequence of column names\n",
        "for column in reversed(df.columns):\n",
        "    \n",
        "    # Select column contents by column\n",
        "    # name using [] operator\n",
        "    columnSeriesObj = df[column]\n",
        "    print('Column Name : ', column)\n",
        "    print('Column Contents : ', columnSeriesObj.values)\n",
        "\n",
        "    Stringval = str(columnSeriesObj.values)\n",
        "    print(Stringval)\n",
        "\n",
        "from pipelines import pipeline\n",
        "nlp = pipeline(\"question-generation\") \n",
        "\n",
        "nlp(Stringval)\n",
        "\n",
        "\n",
        "# Driver code\n",
        "word = nlp(Stringval)\n",
        "#word1 = split(word)\n",
        "#returns only values\n",
        "\n",
        "values = []\n",
        "for dictionary in word:\n",
        "     values.extend([v for k, v in dictionary.items() if k == 'answer'])\n",
        "print(values)\n",
        "\n",
        "def split(word):\n",
        "    return [char for char in word]\n",
        "word2 = split(values)\n",
        "header1 = \"Steps\"\n",
        "header2 = \"Expected_result\"\n",
        "worksheet.write('A1', header1)\n",
        "worksheet.write('B1', header2)\n",
        "\n",
        "row = 1\n",
        "column = 1\n",
        "content = word2\n",
        "  \n",
        "# iterating through content list\n",
        "for item in content :\n",
        "    worksheet.write(row, column, item)\n",
        "    row += 1\n",
        "\n",
        "values1 = []\n",
        "for dictionary in word:\n",
        "     values1.extend([v for k, v in dictionary.items() if k == 'question'])\n",
        "word2 = split(values1)\n",
        "\n",
        "n = [\"happens\",\"do\", \"will\", \"is\", \"are\", \"if\", \"?\", \"[\", \"]\", \"``\", \"'\", \"'\", \"[ \"]     \n",
        "stopwords = set(n)\n",
        "word_tokens = word_tokenize(str(values1))\n",
        "\n",
        "new_strings = [w for w in word_tokens if not w.lower() in stopwords]\n",
        "\n",
        "new_strings = []\n",
        "for string in word_tokens:\n",
        "  if string not in stopwords:\n",
        "     \n",
        "     #Prefix = \"Check\" \n",
        "     #new_string = string.replace(\"What\", \"Check whether\")\n",
        "     new_strings.append(string)\n",
        "     listToStr = ' '.join(new_strings)\n",
        "     listToStr1 = listToStr.split(', ')\n",
        "      \n",
        "append_str = 'Check if '\n",
        "pre_res = [append_str + sub for sub in listToStr1]\n",
        "#suf_res = [sub + append_str for sub in values]\n",
        "  \n",
        "print(\"list after prefix addition : \" + str(pre_res))\n",
        "\n",
        "print(pre_res)\n",
        "\n",
        "row = 1\n",
        "column = 0\n",
        "word3 = pre_res\n",
        "\n",
        "# iterating through content list\n",
        "for item in word3:\n",
        "    worksheet.write_string(row, column, item)\n",
        "    row += 1\n",
        "workbook.close()   \n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Cloning into 'question_generation'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 265 (delta 100), reused 95 (delta 95), pack-reused 156\u001b[K\n",
            "Receiving objects: 100% (265/265), 289.06 KiB | 4.74 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n",
            "/content/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Column Name :  AccpCriteria\n",
            "Column Contents :  ['Given that Diag-diag1 object is created.'\n",
            " 'WHEN Diag-diag1 is opened by admin.'\n",
            " 'And admin is a member of\\xa0 x02xx_PLT_Users'\n",
            " 'THEN CreatenewItem-button should be available to create plant object.'\n",
            " 'When on clicking CreatenewItem-button by admin user.'\n",
            " 'Then New Plant object-Plant1 is created.'\n",
            " 'And Plant object should be in Inwork state.'\n",
            " 'When admin opens the Plant object and selects Promote Button'\n",
            " 'Then Plant-plant1 is promoted to Validated State.']\n",
            "['Given that Diag-diag1 object is created.'\n",
            " 'WHEN Diag-diag1 is opened by admin.'\n",
            " 'And admin is a member of\\xa0 x02xx_PLT_Users'\n",
            " 'THEN CreatenewItem-button should be available to create plant object.'\n",
            " 'When on clicking CreatenewItem-button by admin user.'\n",
            " 'Then New Plant object-Plant1 is created.'\n",
            " 'And Plant object should be in Inwork state.'\n",
            " 'When admin opens the Plant object and selects Promote Button'\n",
            " 'Then Plant-plant1 is promoted to Validated State.']\n",
            "['Diag-diag1 object', 'admin', \"'THEN CreatenewItem-button\", 'CreatenewItem-button', 'New Plant object-Plant1', 'Inwork state', 'Promote Button']\n",
            "list after prefix addition : [\"Check if 'What created by admin \", \"Check if 'Who opens Diag-diag1 \", \"Check if 'What should be available to create plant object \", \"Check if 'When the new plant object-Plant1 created \", \"Check if 'What created when you click CreatenewItem-button \", \"Check if 'What state should plant object be in \", \"Check if 'When admin opens the Plant object and selects what\"]\n",
            "[\"Check if 'What created by admin \", \"Check if 'Who opens Diag-diag1 \", \"Check if 'What should be available to create plant object \", \"Check if 'When the new plant object-Plant1 created \", \"Check if 'What created when you click CreatenewItem-button \", \"Check if 'What state should plant object be in \", \"Check if 'When admin opens the Plant object and selects what\"]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}