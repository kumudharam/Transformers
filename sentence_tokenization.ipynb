{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence-tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumudharam/Transformers/blob/main/sentence_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki06xBcRSuQf",
        "outputId": "6dcbd58b-2b3f-47b7-a547-0035c743f856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install timeline \n",
        "!pip install pipeline\n",
        "!pip install pipelines \n",
        "!pip install openpyxl \n",
        "!pip install xlrd\n",
        "!pip install xlsxwriter "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 28.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 30.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Collecting timeline\n",
            "  Downloading timeline-0.0.7.tar.gz (11 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from timeline) (2018.9)\n",
            "Building wheels for collected packages: timeline\n",
            "  Building wheel for timeline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for timeline: filename=timeline-0.0.7-py3-none-any.whl size=17005 sha256=0cb5cbe0932959af140baadfc15540315778fb687df27afa944afd88b33e6a99\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/03/7d/af6f2e055c7a95e6e9dcd5e64a9738678cac6bd8b05d4e1bd3\n",
            "Successfully built timeline\n",
            "Installing collected packages: timeline\n",
            "Successfully installed timeline-0.0.7\n",
            "Collecting pipeline\n",
            "  Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n",
            "Installing collected packages: pipeline\n",
            "Successfully installed pipeline-0.1.0\n",
            "Collecting pipelines\n",
            "  Downloading pipelines-0.0.14.tar.gz (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 31.9 MB/s \n",
            "\u001b[?25hCollecting futures==3.0.5\n",
            "  Downloading futures-3.0.5.tar.gz (25 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/55/db/97c1ca37edab586a1ae03d6892b6633d8eaa23b23ac40c7e5bbc55423c78/futures-3.0.5.tar.gz#sha256=0542525145d5afc984c88f914a0c85c77527f65946617edb5274f72406f981df (from https://pypi.org/simple/futures/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "Collecting pipelines\n",
            "  Downloading pipelines-0.0.12.tar.gz (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 52.7 MB/s \n",
            "\u001b[?25h  Downloading pipelines-0.0.11.tar.gz (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 34.1 MB/s \n",
            "\u001b[?25h  Downloading pipelines-0.0.10.tar.gz (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 61.3 MB/s \n",
            "\u001b[?25h  Downloading pipelines-0.0.9.tar.gz (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 67.8 MB/s \n",
            "\u001b[?25h  Downloading pipelines-0.0.4.tar.gz (10 kB)\n",
            "  Downloading pipelines-0.0.3.tar.gz (9.6 kB)\n",
            "  Downloading pipelines-0.0.1.tar.gz (8.7 kB)\n",
            "Building wheels for collected packages: pipelines\n",
            "  Building wheel for pipelines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pipelines: filename=pipelines-0.0.1-py3-none-any.whl size=15782 sha256=9d9fedbfac33f63b0f7724280ac14e72009740250879e9deaa06291199c41366\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/8a/50/564066c923abb3bfbafe1cde0d19607085ab4c66f572c1b169\n",
            "Successfully built pipelines\n",
            "Installing collected packages: pipelines\n",
            "Successfully installed pipelines-0.0.1\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.0.1-py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 28.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGchc1tMDfCX",
        "outputId": "479d5861-3a8f-40bf-a891-c8162c0acdc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pandas nltk spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.2.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VCOMiHrEqRQ",
        "outputId": "24a6823b-dd12-476b-e6bd-580455e73c52"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLovth7afRFr"
      },
      "source": [
        "import os\n",
        "\n",
        "# On Jupyter Notebook or Colab\n",
        "DIR_PATH = os.getcwd()\n",
        "# On Python module\n",
        "#DIR_PATH = os.path.dirname(__file__)\n",
        "\n",
        "FILE_PATH = os.path.join(DIR_PATH, \"scripts.csv\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJJrXlJID1bg",
        "outputId": "f02386a3-a6a3-4eb4-e6a5-e06fe09fa2a5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the file\n",
        "df = pd.read_csv(r'/content/TestData.csv')\n",
        "# Remove NaN values\n",
        "df = df[~df[\"Dialogue\"].isna()]\n",
        "# Assign first_dialogue to the first row's \"Dialogue\" column\n",
        "first_dialogue = df.loc[0, \"Dialogue\"]\n",
        "print(first_dialogue)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GIVEN that I'm on any Screen of the application.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HmqACyrSD5-f",
        "outputId": "19500b92-7cdd-44ff-a684-0fbe60d8d5bf"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dialogue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GIVEN that I'm on any Screen of the application.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WHEN I fill the created before as like other f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AND click the Apply filters to get the filtere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THEN the application will display the data bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IF I close the application and Launch again.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Dialogue\n",
              "0   GIVEN that I'm on any Screen of the application.\n",
              "1  WHEN I fill the created before as like other f...\n",
              "2  AND click the Apply filters to get the filtere...\n",
              "3  THEN the application will display the data bas...\n",
              "4      IF I close the application and Launch again. "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkD7e6AdEEQ-",
        "outputId": "abf3e554-e4a2-48e3-d683-aaecddf3bf80"
      },
      "source": [
        "# use Python's split\n",
        "first_dialogue.split(\".\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application\", '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipRRtATkEW7l",
        "outputId": "d9ab1197-0526-4707-8b60-d9c801177353"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_tokenize(first_dialogue)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7PqnpEygfdw",
        "outputId": "b28f9a0d-0ee5-4c55-fbf3-e8b2c82ad0a3"
      },
      "source": [
        "%%timeit -n 10\n",
        "df.loc[:5000, \"Dialogue\"].apply(lambda x: sent_tokenize(x))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 584 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx5WECM2FQIb",
        "outputId": "1e05ea85-d8ae-4309-9bd0-b7c811535ea7"
      },
      "source": [
        "import spacy\n",
        "# use spacy with the dependency parse \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "[str(sent) for sent in nlp(first_dialogue).sents]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWuGSjNdMJI7"
      },
      "source": [
        "#%%timeit -n 10\n",
        "# WARNING: takes a long time!\n",
        "#df.loc[:5000, \"Dialogue\"].apply(lambda x: [sent.text for sent in nlp(x).sents])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToodwNUzLkBZ",
        "outputId": "5a2bb507-4fc1-4b62-97db-1ac9ec5c012a"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "# use spacy with the sentencizer\n",
        "nlp = English()  # just the language with no model\n",
        "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
        "nlp.add_pipe(sentencizer)\n",
        "[str(sent) for sent in nlp(first_dialogue).sents]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pXpftxcnlfz",
        "outputId": "84c797b9-40ac-46ce-a03d-4624f97b6cd9"
      },
      "source": [
        "%%timeit -n 10\n",
        "df.loc[:5000, \"Dialogue\"].apply(lambda x: [sent.text for sent in nlp(x).sents])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 758 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvYfpERJIRKT",
        "outputId": "e0eeb085-38b8-4e0d-e909-2da492bda5d8"
      },
      "source": [
        "import re\n",
        "# use regular expression\n",
        "rule = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
        "re.split(rule, first_dialogue)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"GIVEN that I'm on any Screen of the application.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfSscvTri0Wk",
        "outputId": "a8e481b4-9a26-413e-8c3c-333cf7f825bd"
      },
      "source": [
        "%%timeit -n 10\n",
        "df.loc[:5000, \"Dialogue\"].apply(lambda x: re.split(rule, x))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 361 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxkl0c-YjGPI",
        "outputId": "d68083df-4ec7-4457-eab6-28f2e7d614c7"
      },
      "source": [
        "%%timeit -n 10\n",
        "# Without dependency parser\n",
        "tokenized_data = []\n",
        "first_5000_rows = df.loc[:5000, \"Dialogue\"]\n",
        "for doc in nlp.pipe(first_5000_rows, batch_size=20):\n",
        "  tokenized_data.append([sent.text for sent in doc.sents])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 470 µs per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xQf3YpHhGnj"
      },
      "source": [
        "# Transform data using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# WARNING: takes a long time!\n",
        "df[\"Dialogue\"] = df[\"Dialogue\"].apply(lambda x: [sent.text for sent in nlp(x).sents])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcgRauYoiL9L"
      },
      "source": [
        "df = df.explode(\"Dialogue\", ignore_index=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wgo87wAzR20"
      },
      "source": [
        "df.rename(columns={\"Unnamed: 0\": \"Dialogue ID\"}, inplace=True)\n",
        "df.index.name = \"Sentence ID\""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "5T3ooPLszySR",
        "outputId": "fca1b7ad-78ad-49e3-a078-6e1ae43805c2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dialogue</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sentence ID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GIVEN that I'm on any Screen of the application.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WHEN I fill the created before as like other f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AND click the Apply filters to get the filtere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>THEN the application will display the data bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IF I close the application and Launch again.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Dialogue\n",
              "Sentence ID                                                   \n",
              "0             GIVEN that I'm on any Screen of the application.\n",
              "1            WHEN I fill the created before as like other f...\n",
              "2            AND click the Apply filters to get the filtere...\n",
              "3            THEN the application will display the data bas...\n",
              "4                 IF I close the application and Launch again."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsTZ2qZwz2MX",
        "outputId": "71f7276d-156e-45ca-c4ee-e55ec605e545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Text = df.to_csv(\"scripts_tokenized.csv\")\n",
        "print(df.to_csv(\"scripts_tokenized.csv\"))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA8QjHQkS4Il",
        "outputId": "8e0d8c3f-43a1-4b3b-e491-e25a0fe11118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        }
      },
      "source": [
        "\n",
        "!pip install -U transformers==3.0.0 --quiet\n",
        "!python -m nltk.downloader punkt\n",
        "\n",
        "!git clone https://github.com/patil-suraj/question_generation.git\n",
        "%cd question_generation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from transformers import pipeline\n",
        "import timeline\n",
        "import pandas as pd\n",
        "import csv \n",
        "import openpyxl\n",
        "import xlsxwriter\n",
        "from itertools import chain \n",
        "workbook = xlsxwriter.Workbook('/content/TestData1.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "from pipelines import pipeline\n",
        "nlp = pipeline(\"question-generation\") \n",
        "#Reading from text file\n",
        "#with open('/content/Sample2.txt') as f:\n",
        "    #contents = f.read()\n",
        "    #print(contents)\n",
        "\n",
        "#Reading from xlsx\n",
        "\n",
        "#df = pd.read_excel (r'/content/TestData.xlsx')\n",
        "#print (df)\n",
        "\n",
        "\n",
        "# Read the file\n",
        "df1 = pd.read_csv(r'/content/TestData.csv')\n",
        "#for i in range(len(df)) :\n",
        "  #print(df.loc[i, \"Dialogue\"])\n",
        "  #tester = df.loc[i, \"Dialogue\"]\n",
        "\n",
        "# Convert the dictionary into DataFrame\n",
        "df = pd.DataFrame(df1, columns = ['Dialogue'])\n",
        "  \n",
        "print(\"Given Dataframe :\\n\", df)\n",
        "  \n",
        "print(\"\\nIterating over rows using apply function :\\n\")\n",
        "  \n",
        "# iterate through each row and concatenate\n",
        "# 'Name' and 'Percentage' column respectively.\n",
        "result = []\n",
        "result = [df(\"Dialogue\") for i in result]  \n",
        "# Generate result using pandas\n",
        "#result = []\n",
        "#for value in df[\"Dialogue\"]:\n",
        "    #if value >= 33:\n",
        "       # result.append(value)\n",
        "    #elif value < 0 and value > 100:\n",
        "        #result.append(\"Invalid\")\n",
        "    #else:\n",
        "       # result.append(\"Fail\")\n",
        "#STR = ''\n",
        "#for i in result:\n",
        "   # STR = STR+i     \n",
        "       \n",
        "#tester = STR   \n",
        "print(result)\n",
        "#for line in tester:\n",
        "        #Type = line.split(\",\")\n",
        "\n",
        "nlp(result)\n",
        "#print(getattr(row, \"Dialogue\"))\n",
        "#def split(word):\n",
        "    #return [char for char in word]\n",
        "\n",
        "# Driver code\n",
        "word = nlp(result)\n",
        "word1 = split(word)\n",
        "#returns only values\n",
        "\n",
        "values = []\n",
        "for dictionary in word1:\n",
        "     values.extend([v for k, v in dictionary.items() if k == 'answer'])\n",
        "print(values)\n",
        "\n",
        "word2 = split(values)\n",
        "header1 = \"Steps\"\n",
        "header2 = \"Expected_result\"\n",
        "worksheet.write('A1', header1)\n",
        "worksheet.write('B1', header2)\n",
        "\n",
        "row = 1\n",
        "column = 1\n",
        "content = word2\n",
        "  \n",
        "# iterating through content list\n",
        "for item in content :\n",
        "    worksheet.write(row, column, item)\n",
        "    row += 1\n",
        "\n",
        "values1 = []\n",
        "for dictionary in word1:\n",
        "     values1.extend([v for k, v in dictionary.items() if k == 'question'])\n",
        "word2 = split(values1)\n",
        "\n",
        "n = [\"happens\",\"do\", \"will\", \"is\", \"are\", \"if\", \"?\", \"[\", \"]\", \"``\", \"'\", \"'\", \"[ \"]     \n",
        "stopwords = set(n)\n",
        "word_tokens = word_tokenize(str(values1))\n",
        "\n",
        "new_strings = [w for w in word_tokens if not w.lower() in stopwords]\n",
        "\n",
        "new_strings = []\n",
        "for string in word_tokens:\n",
        "  if string not in stopwords:\n",
        "     \n",
        "     #Prefix = \"Check\" \n",
        "     new_string = string.replace(\"What\", \"Check whether\")\n",
        "     new_strings.append(new_string)\n",
        "     #listToStr = split(new_strings)\n",
        "     listToStr = ' '.join(new_strings)\n",
        "     \n",
        "     listToStr1 = listToStr.split(', ')\n",
        "\n",
        "\n",
        "print(listToStr1)\n",
        "\n",
        "row = 1\n",
        "column = 0\n",
        "word3 = listToStr1\n",
        "\n",
        "# iterating through content list\n",
        "for item in word3:\n",
        "    worksheet.write_string(row, column, item)\n",
        "    row += 1\n",
        "workbook.close()   \n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Cloning into 'question_generation'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 265 (delta 100), reused 95 (delta 95), pack-reused 156\u001b[K\n",
            "Receiving objects: 100% (265/265), 289.06 KiB | 16.06 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n",
            "/content/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Given Dataframe :\n",
            "                                             Dialogue\n",
            "0   GIVEN that I'm on any Screen of the application.\n",
            "1  WHEN I fill the created before as like other f...\n",
            "2  AND click the Apply filters to get the filtere...\n",
            "3  THEN the application will display the data bas...\n",
            "4      IF I close the application and Launch again. \n",
            "5  THEN the recently chosen filters including cre...\n",
            "6   THEN display the corresponding last access page.\n",
            "\n",
            "Iterating over rows using apply function :\n",
            "\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-01c4f42daf7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#Type = line.split(\",\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;31m#print(getattr(row, \"Dialogue\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#def split(word):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/question_generation/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mflat_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO-XEEhSKeGv",
        "outputId": "004a07dc-5ad7-4c8b-8b33-4288898d0913",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U transformers==3.0.0 --quiet\n",
        "!python -m nltk.downloader punkt\n",
        "\n",
        "!git clone https://github.com/patil-suraj/question_generation.git\n",
        "%cd question_generation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from transformers import pipeline\n",
        "import timeline\n",
        "import pandas as pd\n",
        "import csv \n",
        "import openpyxl\n",
        "import xlsxwriter\n",
        "from itertools import chain \n",
        "workbook = xlsxwriter.Workbook('/content/TestData1.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "\n",
        "# Read the file\n",
        "df1 = pd.read_csv(r'/content/TestData.csv')\n",
        "df = pd.DataFrame(df1, columns =['Dialogue'])\n",
        " # Iterate over the sequence of column names\n",
        "for column in reversed(df.columns):\n",
        "    \n",
        "    # Select column contents by column\n",
        "    # name using [] operator\n",
        "    columnSeriesObj = df[column]\n",
        "    print('Column Name : ', column)\n",
        "    print('Column Contents : ', columnSeriesObj.values)\n",
        "\n",
        "    Stringval = str(columnSeriesObj.values)\n",
        "    print(Stringval)\n",
        "\n",
        "from pipelines import pipeline\n",
        "nlp = pipeline(\"question-generation\") \n",
        "\n",
        "nlp(Stringval)\n",
        "\n",
        "\n",
        "# Driver code\n",
        "word = nlp(Stringval)\n",
        "#word1 = split(word)\n",
        "#returns only values\n",
        "\n",
        "values = []\n",
        "for dictionary in word:\n",
        "     values.extend([v for k, v in dictionary.items() if k == 'answer'])\n",
        "print(values)\n",
        "\n",
        "def split(word):\n",
        "    return [char for char in word]\n",
        "word2 = split(values)\n",
        "header1 = \"Steps\"\n",
        "header2 = \"Expected_result\"\n",
        "worksheet.write('A1', header1)\n",
        "worksheet.write('B1', header2)\n",
        "\n",
        "row = 1\n",
        "column = 1\n",
        "content = word2\n",
        "  \n",
        "# iterating through content list\n",
        "for item in content :\n",
        "    worksheet.write(row, column, item)\n",
        "    row += 1\n",
        "\n",
        "values1 = []\n",
        "for dictionary in word:\n",
        "     values1.extend([v for k, v in dictionary.items() if k == 'question'])\n",
        "word2 = split(values1)\n",
        "\n",
        "n = [\"happens\",\"do\", \"will\", \"is\", \"are\", \"if\", \"?\", \"[\", \"]\", \"``\", \"'\", \"'\", \"[ \"]     \n",
        "stopwords = set(n)\n",
        "word_tokens = word_tokenize(str(values1))\n",
        "\n",
        "new_strings = [w for w in word_tokens if not w.lower() in stopwords]\n",
        "\n",
        "new_strings = []\n",
        "for string in word_tokens:\n",
        "  if string not in stopwords:\n",
        "     \n",
        "     #Prefix = \"Check\" \n",
        "     new_string = string.replace(\"What\", \"Check whether\")\n",
        "     new_strings.append(new_string)\n",
        "     #listToStr = split(new_strings)\n",
        "     listToStr = ' '.join(new_strings)\n",
        "     \n",
        "     listToStr1 = listToStr.split(', ')\n",
        "\n",
        "\n",
        "print(listToStr1)\n",
        "\n",
        "row = 1\n",
        "column = 0\n",
        "word3 = listToStr1\n",
        "\n",
        "# iterating through content list\n",
        "for item in word3:\n",
        "    worksheet.write_string(row, column, item)\n",
        "    row += 1\n",
        "workbook.close()   \n",
        "\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Cloning into 'question_generation'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 265 (delta 100), reused 95 (delta 95), pack-reused 156\u001b[K\n",
            "Receiving objects: 100% (265/265), 289.06 KiB | 17.00 MiB/s, done.\n",
            "Resolving deltas: 100% (146/146), done.\n",
            "/content/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation/question_generation\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Column Name :  Dialogue\n",
            "Column Contents :  [\"GIVEN that I'm on any Screen of the application.\"\n",
            " 'WHEN I fill the created before as like other filters.'\n",
            " 'AND click the Apply filters to get the filtered value from the backend.'\n",
            " 'THEN the application will display the data based on the applied filters. '\n",
            " 'IF I close the application and Launch again. '\n",
            " 'THEN the recently chosen filters including created before will be applied automatically from the local storage.'\n",
            " 'THEN display the corresponding last access page.']\n",
            "[\"GIVEN that I'm on any Screen of the application.\"\n",
            " 'WHEN I fill the created before as like other filters.'\n",
            " 'AND click the Apply filters to get the filtered value from the backend.'\n",
            " 'THEN the application will display the data based on the applied filters. '\n",
            " 'IF I close the application and Launch again. '\n",
            " 'THEN the recently chosen filters including created before will be applied automatically from the local storage.'\n",
            " 'THEN display the corresponding last access page.']\n",
            "['any Screen of the application', 'fill the created before as like other filters', 'Apply filters', 'the application will display the data based on the applied filters', 'Launch again', 'created before', 'last access page']\n",
            "[\"Check whether the name of the screen that I 'm on \", \"Check whether I when I 'm on any Screen of the application \", \"'Check whether get the filtered value from the backend \", \"'Check whether the application display the data based on \", \"'Check whether I close the application \", \"'Check whether be applied automatically from the local storage \", \"Check whether page does 'THEN display ''\"]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}