{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "QuestionAnsweringUsingT5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumudharam/Transformers/blob/main/QuestionAnsweringUsingT5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qd1egoJXHAG"
      },
      "source": [
        "# check for the GPU provided in the runtime\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-28tCMuZIn0"
      },
      "source": [
        "# using quiet method for controlling the log\n",
        "# for suppressing the colored errors and warning in the terminal\n",
        "!pip install --quiet transformers==4.1.1\n",
        "# pytorch lightning for smoother model training and data loading\n",
        "!pip install --quiet https://github.com/PyTorchLightning/pytorch-lightning/releases/download/1.2.6/pytorch-lightning-1.2.6.tar.gz \n",
        "# using HuggingFace tokenizers\n",
        "!pip install --quiet tokenizers==0.9.4\n",
        "# Google's sentencepiece\n",
        "!pip install --quiet sentencepiece==0.1.94"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VswKj_dSqsY5"
      },
      "source": [
        "# mostly pl is used while doing complex model training\n",
        "import pytorch_lightning as pl\n",
        "print(pl.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HRG-0vqZkbx"
      },
      "source": [
        "# argparse makes it easier to write user friendly command line interfaces\n",
        "import argparse\n",
        "# package for faster file name matching\n",
        "import glob\n",
        "# makiing directories for data \n",
        "import os\n",
        "# reading json files as the data is present in json files\n",
        "import json\n",
        "# time module for calculating the model runtime\n",
        "import time\n",
        "# Allows writing status messages to a file\n",
        "import logging\n",
        "# generate random float numbers uniformly\n",
        "import random\n",
        "# regex module for text \n",
        "import re\n",
        "# module provides various functions which work on \n",
        "# iterators too produce complex iterators\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "# pandas for data manipulation\n",
        "import pandas as pd\n",
        "# numpy for array operations\n",
        "import numpy as np\n",
        "# PyTorch\n",
        "import torch\n",
        "# provides various classes representing file system paths\n",
        "# with appropriate semantics\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# splitting the data \n",
        "from sklearn.model_selection import train_test_split\n",
        "# ANSII color formatting for ouput in terminal\n",
        "from termcolor import colored\n",
        "# wrapping paragraphs into string\n",
        "import textwrap\n",
        "\n",
        "# model checkpoints in pretrained model\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "'''\n",
        "optimizer - AdamW\n",
        "T5 Conditional Generator in which we'll give conditions\n",
        "T5 tokenizer because it is fast\n",
        "training the model without a learning rate\n",
        "'''\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g9RyS26ao6N"
      },
      "source": [
        "# Seeds all the processes including numpy torch and other imported modules.\n",
        "pl.seed_everything(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLwmdXZwYBBD"
      },
      "source": [
        "# check the version provided by Lightning\n",
        "import pytorch_lightning as pl\n",
        "print(pl.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GZAifg3a5qL"
      },
      "source": [
        "# QA dataset from https://github.com/dmis-lab/bioasq-biobert\n",
        "# which is in Zip format\n",
        "!gdown --id 1mxVUywvKzvA9bvrUc11RYuOTy7MYcXHF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC2mQLy5bR81"
      },
      "source": [
        "# Unzipping the folder\n",
        "!unzip -q bio-QA.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R64t0s7qfYRR"
      },
      "source": [
        "# let's have a look at one of the files\n",
        "with Path(\"BioASQ/BioASQ-train-factoid-4b.json\").open() as json_file:\n",
        "  data = json.load(json_file)                                            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50xWA1Daf20a"
      },
      "source": [
        "# Data is a dictionary\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzJG77btgZai"
      },
      "source": [
        "data['version']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTtqg76Fgbx9"
      },
      "source": [
        "# len of each file\n",
        "len(data['data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcYfBhQErm0D"
      },
      "source": [
        "# We have a list of dictionaries in the \"data\". We can explore the 0th element\n",
        "data['data'][0].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8qQqmUvggNp"
      },
      "source": [
        "data['data'][0]['title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pwN0LzAr5WF"
      },
      "source": [
        "len(data['data'][0]['paragraphs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osJEpTsIgnjJ"
      },
      "source": [
        "questions = data['data'][0]['paragraphs']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd6Ps2tlgym-"
      },
      "source": [
        "# datapoint sample\n",
        "questions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLafI4AstqzH"
      },
      "source": [
        "# Function to Create a pandas dataframes of questions and answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzddd0tug2wR"
      },
      "source": [
        "def extract_questions_and_answers(factoid_path = Path):\n",
        "  with factoid_path.open() as json_file:\n",
        "    data = json.load(json_file)\n",
        "    questions = data['data'][0]['paragraphs']\n",
        "    data_rows = []\n",
        "    for question in questions:\n",
        "      context = question['context']\n",
        "      for question_and_answers in question['qas']:\n",
        "        question = question_and_answers['question']\n",
        "        answers = question_and_answers['answers']\n",
        "        for answer in answers:\n",
        "          answer_text = answer['text']\n",
        "          answer_start = answer['answer_start']\n",
        "          answer_end = answer['answer_start'] + len(answer_text)  #Gets the end index of each answer in the paragraph\n",
        "          \n",
        "          data_rows.append({\n",
        "                \"question\" : question,\n",
        "                \"context\"  : context,\n",
        "                \"answer_text\" : answer_text,\n",
        "                \"answer_start\" : answer_start,\n",
        "                \"answer_end\" : answer_end\n",
        "            })\n",
        "    return pd.DataFrame(data_rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3kZ3Tl6kX6G"
      },
      "source": [
        "factoid_path = Path(\"BioASQ/BioASQ-train-factoid-4b.json\")\n",
        "extract_questions_and_answers(factoid_path).head()      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vTQ8Ioakegk"
      },
      "source": [
        "factoid_paths = sorted(list(Path('BioASQ/').glob('BioASQ-train-*')))\n",
        "factoid_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHtWx7H6mata"
      },
      "source": [
        "dfs = []\n",
        "\n",
        "for factoid_path in factoid_paths:\n",
        "  df = extract_questions_and_answers(factoid_path)\n",
        "  dfs.append(df)\n",
        "\n",
        "df = pd.concat(dfs)\n",
        "\n",
        "dfs = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wynxDnMmawR"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8SO3M9vmJqL"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QveL85kXKZXS"
      },
      "source": [
        "# Dropping all the rows with repeated context and questions pairs.\n",
        "\n",
        "df = df.drop_duplicates(subset=[\"context\"]).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmHSVE-8KZaD"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlJ1Nq0Wm3Sb"
      },
      "source": [
        "len(df.question.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlABCK63m-Xe"
      },
      "source": [
        "len(df.context.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HVNoMGrnBLs"
      },
      "source": [
        "sample_question = df.iloc[243]\n",
        "sample_question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2I78NRWnNN3"
      },
      "source": [
        "# Using textcolor to visualize the answer within the context\n",
        "\n",
        "def color_answer(question):\n",
        "  answer_start, answer_end = question[\"answer_start\"],question[\"answer_end\"]\n",
        "  context = question['context']\n",
        "\n",
        "  return  colored(context[:answer_start], \"white\") + \\\n",
        "    colored(context[answer_start:answer_end + 1], \"green\") + \\\n",
        "    colored(context[answer_end+1:], \"white\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sDkGTTUoOa3"
      },
      "source": [
        "print(sample_question['question'])\n",
        "print()\n",
        "print(\"Answer: \")\n",
        "for wrap in textwrap.wrap(color_answer(sample_question), width = 100):\n",
        "  print(wrap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-rvJMpNt5Zo"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ozZ4ptoYAN"
      },
      "source": [
        "# using the base T5 model having 222M params\n",
        "MODEL_NAME ='t5-base'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrtKKzO8pHAL"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKxWSsdupHC2"
      },
      "source": [
        "sample_encoding = tokenizer('is the glass half empty or half full?', 'It depends on the initial state of the glass. If the glass starts out empty and liquid is added until it is half full, it is half full. If the glass starts out full and liquid is removed until it is half empty, it is half empty.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83i_WzPGpyJR"
      },
      "source": [
        "sample_encoding.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWJlEbtLp1ap"
      },
      "source": [
        "print(sample_encoding[\"input_ids\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVEOLW6CqCCD"
      },
      "source": [
        "print(sample_encoding[\"attention_mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44fV5Mauvp-F"
      },
      "source": [
        "print(len(sample_encoding['input_ids']), len(sample_encoding['attention_mask']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIy8dTAJqFW0"
      },
      "source": [
        "# Checking the decoding of the input ids\n",
        "\n",
        "preds = [\n",
        "         tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "         for input_id in sample_encoding['input_ids']\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi1xjpSVqeYc"
      },
      "source": [
        "preds= \" \".join(preds)\n",
        "for wrap in textwrap.wrap(preds, width = 80):\n",
        "  print(wrap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHFoy6akwCAi"
      },
      "source": [
        "There exists a special seperator token in between the question and its answers.\n",
        "\n",
        "Checking the encoding on the sample question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2_zfw8xqgBf"
      },
      "source": [
        "encoding = tokenizer(\n",
        "    sample_question['question'],\n",
        "    sample_question['context'],\n",
        "    max_length=396,\n",
        "    padding='max_length',\n",
        "    truncation=\"only_second\",\n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNgUeiafsQPX"
      },
      "source": [
        "encoding.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TCcHThUsXgt"
      },
      "source": [
        "tokenizer.special_tokens_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myaSEq9vsd20"
      },
      "source": [
        "tokenizer.eos_token, tokenizer.eos_token_id\n",
        "# Input id of 1 represents end of sequence token."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND-qMS3MsssL"
      },
      "source": [
        "# Text representation pf the input ids\n",
        "\n",
        "tokenizer.decode(encoding['input_ids'].squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O94b5A7wfMw"
      },
      "source": [
        "## Creating the labels for the answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TArh_28mtGRC"
      },
      "source": [
        "answer_encoding = tokenizer(\n",
        "    sample_question['answer_text'],\n",
        "    max_length=32,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MhbXR76tZJK"
      },
      "source": [
        "tokenizer.decode(answer_encoding['input_ids'].squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3q5mC44tfSx"
      },
      "source": [
        "labels = answer_encoding[\"input_ids\"]\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PiJpR5cwuPI"
      },
      "source": [
        "Labels after the end of sequence in the answer encoding has to be converted to -100 from 0 for the model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocbgT0JDueVZ"
      },
      "source": [
        "labels[labels == 0] = -100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qIroGAVulLI"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0OrBRcPuodP"
      },
      "source": [
        "## To create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC84KMKxuqFN"
      },
      "source": [
        "class BioQADataset(Dataset):\n",
        "  def __init__(\n",
        "      self,\n",
        "      data:pd.DataFrame,\n",
        "      tokenizer:T5Tokenizer,\n",
        "      source_max_token_len: int = 396,\n",
        "      target_max_token_len: int = 32,\n",
        "\n",
        "      ):\n",
        "    \n",
        "    self.data =  data\n",
        "    self.tokenizer =  tokenizer\n",
        "    self.source_max_token_len =  source_max_token_len\n",
        "    self.target_max_token_len =  target_max_token_len\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    data_row = self.data.iloc[index]\n",
        "\n",
        "    source_encoding = tokenizer(\n",
        "      data_row['question'],\n",
        "      data_row['context'],\n",
        "      max_length=self.source_max_token_len,\n",
        "      padding='max_length',\n",
        "      truncation=\"only_second\",\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "      )\n",
        "    \n",
        "    target_encoding = tokenizer(\n",
        "      data_row['answer_text'],\n",
        "      max_length=self.target_max_token_len,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "      )\n",
        "    \n",
        "    labels = target_encoding['input_ids']\n",
        "    labels[labels==0] = -100\n",
        "\n",
        "    return dict(\n",
        "        question=data_row['question'],\n",
        "        context=data_row['context'],\n",
        "        answer_text=data_row['answer_text'],\n",
        "        input_ids=source_encoding[\"input_ids\"].flatten(),\n",
        "        attention_mask=source_encoding['attention_mask'].flatten(),\n",
        "        labels=labels.flatten()\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI-CMUtuxkZz"
      },
      "source": [
        "sample_dataset = BioQADataset(df, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIfXyyObx3Kq"
      },
      "source": [
        "for data in sample_dataset:\n",
        "  print(\"Question: \", data['question'])\n",
        "  print(\"Answer text: \", data['answer_text'])\n",
        "  print(\"Input_ids: \", data['input_ids'][:10])\n",
        "  print(\"Labels: \", data['labels'][:10])\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD8JFfYJxCcl"
      },
      "source": [
        "## Splitting into train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXOHeMUVyIYS"
      },
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wv5RMMlyyvs"
      },
      "source": [
        "train_df.shape,  val_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uyNj_lcy73E"
      },
      "source": [
        "# Create pytorch lightning datamodule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLGivrpDy9ph"
      },
      "source": [
        "class BioDataModule(pl.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      train_df: pd.DataFrame,\n",
        "      test_df: pd.DataFrame,\n",
        "      tokenizer:T5Tokenizer,\n",
        "      batch_size: int = 8,\n",
        "      source_max_token_len: int = 396,\n",
        "      target_max_token_len: int = 32,\n",
        "      ):\n",
        "    super().__init__()\n",
        "    self.train_df = train_df\n",
        "    self.test_df = test_df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.batch_size = batch_size\n",
        "    self.source_max_token_len = source_max_token_len\n",
        "    self.target_max_token_len = target_max_token_len\n",
        "\n",
        "  def setup(self):\n",
        "    self.train_dataset = BioQADataset(\n",
        "        self.train_df,\n",
        "        self.tokenizer,\n",
        "        self.source_max_token_len,\n",
        "        self.target_max_token_len\n",
        "        )\n",
        "\n",
        "    self.test_dataset = BioQADataset(\n",
        "    self.test_df,\n",
        "    self.tokenizer,\n",
        "    self.source_max_token_len,\n",
        "    self.target_max_token_len\n",
        "    )\n",
        " \n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "        )\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=self.batch_size,\n",
        "        num_workers=4\n",
        "        )\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.test_dataset,\n",
        "        batch_size=1,\n",
        "        num_workers=4\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCgqZshX2CLy"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "N_EPOCHS = 6\n",
        "\n",
        "data_module = BioDataModule(train_df, val_df, tokenizer, batch_size=BATCH_SIZE)\n",
        "data_module.setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og0JGVoIxdce"
      },
      "source": [
        "## Loading and finetuning the T5-base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_hs16xS2Zq8"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOxM8e7M3tMv"
      },
      "source": [
        "model.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMSse28ENDJg"
      },
      "source": [
        "# To check the translation from English to German built-in task \n",
        "\n",
        "input_ids_translated = tokenizer(\n",
        "    \"translate English to German : Oppertunity did not knock until I built a door\",\n",
        "    return_tensors = 'pt'\n",
        ").input_ids\n",
        "\n",
        "generated_ids = model.generate(input_ids = input_ids_translated)\n",
        "generated_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QezxvcDIN14k"
      },
      "source": [
        "pred_translated = [\n",
        "         tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "         for gen_id in generated_ids\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoaJkN4qOMPc"
      },
      "source": [
        "pred_translated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvTM_jA0ONTl"
      },
      "source": [
        "\"\".join(pred_translated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8yr7JCjOr3W"
      },
      "source": [
        "# To check the summarization built-in task\n",
        "\n",
        "text = \"\"\"summarize : Musk was born to a Canadian mother and South African father and raised in Pretoria, South Africa. \n",
        "He briefly attended the University of Pretoria before moving to Canada aged 17 to attend Queen's University. \n",
        "He transferred to the University of Pennsylvania two years later, where he received bachelor's degrees in economics and physics. \n",
        "He moved to California in 1995 to attend Stanford University but decided instead to pursue a business career, \n",
        "co-founding the web software company Zip2 with his brother Kimbal. The startup was acquired by Compaq for $307 million in 1999. \n",
        "Musk co-founded online bank X.com that same year, which merged with Confinity in 2000 to form PayPal. \n",
        "The company was bought by eBay in 2002 for $1.5 billion. In 2002, Musk founded SpaceX, an aerospace manufacturer and space transport \n",
        "services company, of which he is CEO and CTO. In 2004, he joined electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.) \n",
        "as chairman and product architect, becoming its CEO in 2008. In 2006, he helped create SolarCity, a solar energy services company that \n",
        "was later acquired by Tesla and became Tesla Energy. In 2015, he co-founded OpenAI, a nonprofit research company that promotes friendly \n",
        "artificial intelligence. In 2016, he co-founded Neuralink, a neurotechnology company focused on developing brain–computer interfaces, \n",
        "and founded The Boring Company, a tunnel construction company. Musk has proposed the Hyperloop, a high-speed vactrain transportation system.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8B0RD3FPa7C"
      },
      "source": [
        "input_ids_summary = tokenizer(\n",
        "    text,\n",
        "    return_tensors = 'pt'\n",
        ").input_ids\n",
        "\n",
        "generated_ids_summary = model.generate(input_ids = input_ids_summary)\n",
        "generated_ids_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqM7oiwVPw06"
      },
      "source": [
        "pred_summary = [\n",
        "         tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "         for gen_id in generated_ids\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65hVQpc2PuFs"
      },
      "source": [
        "\" \".join(pred_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw7eUdVoPxZK"
      },
      "source": [
        "# Model config\n",
        "\n",
        "model.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQz9YsjYQ65O"
      },
      "source": [
        "output = model(\n",
        "    input_ids = encoding['input_ids'],\n",
        "    attention_mask = encoding['attention_mask'],\n",
        "    labels = labels\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl4CYfitSFrS"
      },
      "source": [
        "output.logits.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsGP25zgSJY-"
      },
      "source": [
        "output.loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQCsI_RSrsI"
      },
      "source": [
        "## Building the PyTorch lightning module using T5ForConditionalGeneration model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yu2hpq8SLOi"
      },
      "source": [
        "class BioQAModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    output = self.model(\n",
        "        input_ids, \n",
        "        attention_mask=attention_mask,\n",
        "        labels=labels)\n",
        "\n",
        "    return output.loss, output.logits\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "    return {\"loss\": loss, \"predictions\":outputs, \"labels\": labels}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask=batch['attention_mask']\n",
        "    labels = batch['labels']\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    optimizer = AdamW(self.parameters(), lr=0.0001)\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX2RY1vK1YVr"
      },
      "source": [
        "model = BioQAModel() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG9H_2goykbu"
      },
      "source": [
        "## Using trainer from pytorch lightning to finetune model using our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk8jqylDVPXh"
      },
      "source": [
        "model = BioQAModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2kzoAS_Vcsd"
      },
      "source": [
        "# To record the best performing model using checkpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"checkpoints\",\n",
        "    filename=\"best-checkpoint\",\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6QnnPRGUxpq"
      },
      "source": [
        "#logger = TensorBoardLogger(\"training-logs\", name=\"bio-qa\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghzCmhl5W2Vi"
      },
      "source": [
        "#logger = TensorBoardLogger(\"training-logs\", name=\"bio-qa\")\n",
        "trainer = pl.Trainer(\n",
        "    #logger = logger,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    max_epochs=N_EPOCHS,\n",
        "    gpus=1,\n",
        "    progress_bar_refresh_rate = 30\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTX6GRpMzFh0"
      },
      "source": [
        "## Loading Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0DbRcMQYBFA"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co6GUVELYIwM"
      },
      "source": [
        "%tensorboard --logdir ./lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtmszuLGyFO4"
      },
      "source": [
        "#!rm --rf lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLVpNh9hYSIF"
      },
      "source": [
        "trainer.fit(model, data_module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDAbkxtQYxzb"
      },
      "source": [
        "trainer.test()  # evaluate the model according to the last checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Bot5Waz6F_"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPH2sKbIz7uc"
      },
      "source": [
        "trained_model = BioQAModel.load_from_checkpoint(\"checkpoints/best-checkpoint.ckpt\")\n",
        "trained_model.freeze() # "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIOml8Oe9kPb"
      },
      "source": [
        "## Generate answers for the questions in the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2uEi6rv0Oan"
      },
      "source": [
        "def generate_answer(question):\n",
        "  source_encoding=tokenizer(\n",
        "      question[\"question\"],\n",
        "      question['context'],\n",
        "      max_length = 396,\n",
        "      padding=\"max_length\",\n",
        "      truncation=\"only_second\",\n",
        "      return_attention_mask=True,\n",
        "      add_special_tokens=True,\n",
        "      return_tensors=\"pt\"\n",
        "\n",
        "  )\n",
        "\n",
        "  generated_ids = trained_model.model.generate(\n",
        "      input_ids=source_encoding[\"input_ids\"],\n",
        "      attention_mask=source_encoding[\"attention_mask\"],\n",
        "      num_beams=1,  # greedy search\n",
        "      max_length=80,\n",
        "      repetition_penalty=2.5,\n",
        "      early_stopping=True,\n",
        "      use_cache=True)\n",
        "  \n",
        "  preds = [\n",
        "          tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "          for generated_id in generated_ids\n",
        "  ]\n",
        "\n",
        "  return \"\".join(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TViaCgYM5h44"
      },
      "source": [
        "sample_question = val_df.iloc[20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38zxGKgzJ5c3"
      },
      "source": [
        "sample_question[\"question\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef6YIfM2E4ib"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvBwizLcJ9jm"
      },
      "source": [
        "sample_question[\"answer_text\"]  # Label Answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igRYEwO_KCmG"
      },
      "source": [
        "generate_answer(sample_question)  # Predicted answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REdkokA6KIBf"
      },
      "source": [
        "sample_question = val_df.iloc[66]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwHbMQ_3K4HI"
      },
      "source": [
        "sample_question[\"answer_text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVVCDmBBKxXC"
      },
      "source": [
        "generate_answer(sample_question)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyWKE9z3K78z"
      },
      "source": [
        "sample_question = val_df.iloc[114]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbRqkQcdK0EA"
      },
      "source": [
        "sample_question[\"answer_text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UlN6x6BK9DY"
      },
      "source": [
        "generate_answer(sample_question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kSHUaEBkmKj"
      },
      "source": [
        "#mkdir zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcVzZ_ZFK_QL"
      },
      "source": [
        "!zip -r /content.zip /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_x-6xw4n7f4"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXvdVQMmxKV5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}