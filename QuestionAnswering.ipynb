{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionAnswering.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSALEg1xVb61utQHq3Zwyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumudharam/Transformers/blob/main/QuestionAnswering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er79A3b4XRyM",
        "outputId": "61261de8-3515-4fd7-dfeb-1e427c0e6938"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 77.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "qGL1GvzzXSP2",
        "outputId": "d009f93c-c5ca-4b97-9c0f-a0a5e67d2a1c"
      },
      "source": [
        "from collections.abc import Iterable\n",
        "from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from ..data import SquadExample, SquadFeatures, squad_convert_examples_to_features\n",
        "from ..file_utils import PaddingStrategy, add_end_docstrings, is_tf_available, is_torch_available\n",
        "from ..modelcard import ModelCard\n",
        "from ..tokenization_utils import PreTrainedTokenizer\n",
        "from .base import PIPELINE_INIT_ARGS, ArgumentHandler, Pipeline\n",
        "\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    from ..modeling_tf_utils import TFPreTrainedModel\n",
        "    from ..modeling_utils import PreTrainedModel\n",
        "\n",
        "if is_tf_available():\n",
        "    import tensorflow as tf\n",
        "\n",
        "    from ..models.auto.modeling_tf_auto import TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
        "\n",
        "if is_torch_available():\n",
        "    import torch\n",
        "\n",
        "    from ..models.auto.modeling_auto import MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
        "\n",
        "\n",
        "#[DOCS]\n",
        "class QuestionAnsweringArgumentHandler(ArgumentHandler):\n",
        "    \"\"\"\n",
        "    QuestionAnsweringPipeline requires the user to provide multiple arguments (i.e. question & context) to be mapped to\n",
        "    internal :class:`~transformers.SquadExample`.\n",
        "\n",
        "    QuestionAnsweringArgumentHandler manages all the possible to create a :class:`~transformers.SquadExample` from the\n",
        "    command-line supplied arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    def normalize(self, item):\n",
        "        if isinstance(item, SquadExample):\n",
        "            return item\n",
        "        elif isinstance(item, dict):\n",
        "            for k in [\"question\", \"context\"]:\n",
        "                if k not in item:\n",
        "                    raise KeyError(\"You need to provide a dictionary with keys {question:..., context:...}\")\n",
        "                elif item[k] is None:\n",
        "                    raise ValueError(f\"`{k}` cannot be None\")\n",
        "                elif isinstance(item[k], str) and len(item[k]) == 0:\n",
        "                    raise ValueError(f\"`{k}` cannot be empty\")\n",
        "\n",
        "            return QuestionAnsweringPipeline.create_sample(**item)\n",
        "        raise ValueError(f\"{item} argument needs to be of type (SquadExample, dict)\")\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        # Detect where the actual inputs are\n",
        "        if args is not None and len(args) > 0:\n",
        "            if len(args) == 1:\n",
        "                inputs = args[0]\n",
        "            elif len(args) == 2 and {type(el) for el in args} == {str}:\n",
        "                inputs = [{\"question\": args[0], \"context\": args[1]}]\n",
        "            else:\n",
        "                inputs = list(args)\n",
        "        # Generic compatibility with sklearn and Keras\n",
        "        # Batched data\n",
        "        elif \"X\" in kwargs:\n",
        "            inputs = kwargs[\"X\"]\n",
        "        elif \"data\" in kwargs:\n",
        "            inputs = kwargs[\"data\"]\n",
        "        elif \"question\" in kwargs and \"context\" in kwargs:\n",
        "            if isinstance(kwargs[\"question\"], list) and isinstance(kwargs[\"context\"], str):\n",
        "                inputs = [{\"question\": Q, \"context\": kwargs[\"context\"]} for Q in kwargs[\"question\"]]\n",
        "            elif isinstance(kwargs[\"question\"], list) and isinstance(kwargs[\"context\"], list):\n",
        "                if len(kwargs[\"question\"]) != len(kwargs[\"context\"]):\n",
        "                    raise ValueError(\"Questions and contexts don't have the same lengths\")\n",
        "\n",
        "                inputs = [{\"question\": Q, \"context\": C} for Q, C in zip(kwargs[\"question\"], kwargs[\"context\"])]\n",
        "            elif isinstance(kwargs[\"question\"], str) and isinstance(kwargs[\"context\"], str):\n",
        "                inputs = [{\"question\": kwargs[\"question\"], \"context\": kwargs[\"context\"]}]\n",
        "            else:\n",
        "                raise ValueError(\"Arguments can't be understood\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown arguments {kwargs}\")\n",
        "\n",
        "        # Normalize inputs\n",
        "        if isinstance(inputs, dict):\n",
        "            inputs = [inputs]\n",
        "        elif isinstance(inputs, Iterable):\n",
        "            # Copy to avoid overriding arguments\n",
        "            inputs = [i for i in inputs]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid arguments {kwargs}\")\n",
        "\n",
        "        for i, item in enumerate(inputs):\n",
        "            inputs[i] = self.normalize(item)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@add_end_docstrings(PIPELINE_INIT_ARGS)\n",
        "class QuestionAnsweringPipeline(Pipeline):\n",
        "    \"\"\"\n",
        "    Question Answering pipeline using any :obj:`ModelForQuestionAnswering`. See the `question answering examples\n",
        "    <../task_summary.html#question-answering>`__ for more information.\n",
        "\n",
        "    This question answering pipeline can currently be loaded from :func:`~transformers.pipeline` using the following\n",
        "    task identifier: :obj:`\"question-answering\"`.\n",
        "\n",
        "    The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\n",
        "    up-to-date list of available models on `huggingface.co/models\n",
        "    <https://huggingface.co/models?filter=question-answering>`__.\n",
        "    \"\"\"\n",
        "\n",
        "    default_input_names = \"question,context\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Union[\"PreTrainedModel\", \"TFPreTrainedModel\"],\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        modelcard: Optional[ModelCard] = None,\n",
        "        framework: Optional[str] = None,\n",
        "        device: int = -1,\n",
        "        task: str = \"\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            modelcard=modelcard,\n",
        "            framework=framework,\n",
        "            device=device,\n",
        "            task=task,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self._args_parser = QuestionAnsweringArgumentHandler()\n",
        "        self.check_model_type(\n",
        "            TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING if self.framework == \"tf\" else MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
        "        )\n",
        "\n",
        "   \n",
        "    @staticmethod\n",
        "    def create_sample(\n",
        "        question: Union[str, List[str]], context: Union[str, List[str]]\n",
        "    ) -> Union[SquadExample, List[SquadExample]]:\n",
        "        \"\"\"\n",
        "        QuestionAnsweringPipeline leverages the :class:`~transformers.SquadExample` internally. This helper method\n",
        "        encapsulate all the logic for converting question(s) and context(s) to :class:`~transformers.SquadExample`.\n",
        "\n",
        "        We currently support extractive question answering.\n",
        "\n",
        "        Arguments:\n",
        "            question (:obj:`str` or :obj:`List[str]`): The question(s) asked.\n",
        "            context (:obj:`str` or :obj:`List[str]`): The context(s) in which we will look for the answer.\n",
        "\n",
        "        Returns:\n",
        "            One or a list of :class:`~transformers.SquadExample`: The corresponding :class:`~transformers.SquadExample`\n",
        "            grouping question and context.\n",
        "        \"\"\"\n",
        "        if isinstance(question, list):\n",
        "            return [SquadExample(None, q, c, None, None, None) for q, c in zip(question, context)]\n",
        "        else:\n",
        "            return SquadExample(None, question, context, None, None, None)\n",
        "\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Answer the question(s) given as inputs by using the context(s).\n",
        "\n",
        "        Args:\n",
        "            args (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`):\n",
        "                One or several :class:`~transformers.SquadExample` containing the question and context.\n",
        "            X (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`, `optional`):\n",
        "                One or several :class:`~transformers.SquadExample` containing the question and context (will be treated\n",
        "                the same way as if passed as the first positional argument).\n",
        "            data (:class:`~transformers.SquadExample` or a list of :class:`~transformers.SquadExample`, `optional`):\n",
        "                One or several :class:`~transformers.SquadExample` containing the question and context (will be treated\n",
        "                the same way as if passed as the first positional argument).\n",
        "            question (:obj:`str` or :obj:`List[str]`):\n",
        "                One or several question(s) (must be used in conjunction with the :obj:`context` argument).\n",
        "            context (:obj:`str` or :obj:`List[str]`):\n",
        "                One or several context(s) associated with the question(s) (must be used in conjunction with the\n",
        "                :obj:`question` argument).\n",
        "            topk (:obj:`int`, `optional`, defaults to 1):\n",
        "                The number of answers to return (will be chosen by order of likelihood). Note that we return less than\n",
        "                topk answers if there are not enough options available within the context.\n",
        "            doc_stride (:obj:`int`, `optional`, defaults to 128):\n",
        "                If the context is too long to fit with the question for the model, it will be split in several chunks\n",
        "                with some overlap. This argument controls the size of that overlap.\n",
        "            max_answer_len (:obj:`int`, `optional`, defaults to 15):\n",
        "                The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n",
        "            max_seq_len (:obj:`int`, `optional`, defaults to 384):\n",
        "                The maximum length of the total sentence (context + question) after tokenization. The context will be\n",
        "                split in several chunks (using :obj:`doc_stride`) if needed.\n",
        "            max_question_len (:obj:`int`, `optional`, defaults to 64):\n",
        "                The maximum length of the question after tokenization. It will be truncated if needed.\n",
        "            handle_impossible_answer (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "                Whether or not we accept impossible as an answer.\n",
        "\n",
        "        Return:\n",
        "            A :obj:`dict` or a list of :obj:`dict`: Each result comes as a dictionary with the following keys:\n",
        "\n",
        "            - **score** (:obj:`float`) -- The probability associated to the answer.\n",
        "            - **start** (:obj:`int`) -- The character start index of the answer (in the tokenized version of the\n",
        "              input).\n",
        "            - **end** (:obj:`int`) -- The character end index of the answer (in the tokenized version of the input).\n",
        "            - **answer** (:obj:`str`) -- The answer to the question.\n",
        "        \"\"\"\n",
        "        # Set defaults values\n",
        "        kwargs.setdefault(\"padding\", \"longest\")\n",
        "        kwargs.setdefault(\"topk\", 1)\n",
        "        kwargs.setdefault(\"doc_stride\", 128)\n",
        "        kwargs.setdefault(\"max_answer_len\", 15)\n",
        "        kwargs.setdefault(\"max_seq_len\", 384)\n",
        "        kwargs.setdefault(\"max_question_len\", 64)\n",
        "        kwargs.setdefault(\"handle_impossible_answer\", False)\n",
        "\n",
        "        if kwargs[\"topk\"] < 1:\n",
        "            raise ValueError(f\"topk parameter should be >= 1 (got {kwargs['topk']})\")\n",
        "\n",
        "        if kwargs[\"max_answer_len\"] < 1:\n",
        "            raise ValueError(f\"max_answer_len parameter should be >= 1 (got {(kwargs['max_answer_len'])}\")\n",
        "\n",
        "        # Convert inputs to features\n",
        "        examples = self._args_parser(*args, **kwargs)\n",
        "        if not self.tokenizer.is_fast:\n",
        "            features_list = [\n",
        "                squad_convert_examples_to_features(\n",
        "                    examples=[example],\n",
        "                    tokenizer=self.tokenizer,\n",
        "                    max_seq_length=kwargs[\"max_seq_len\"],\n",
        "                    doc_stride=kwargs[\"doc_stride\"],\n",
        "                    max_query_length=kwargs[\"max_question_len\"],\n",
        "                    padding_strategy=PaddingStrategy.MAX_LENGTH.value,\n",
        "                    is_training=False,\n",
        "                    tqdm_enabled=False,\n",
        "                )\n",
        "                for example in examples\n",
        "            ]\n",
        "        else:\n",
        "            features_list = []\n",
        "            for example in examples:\n",
        "                # Define the side we want to truncate / pad and the text/pair sorting\n",
        "                question_first = bool(self.tokenizer.padding_side == \"right\")\n",
        "\n",
        "                encoded_inputs = self.tokenizer(\n",
        "                    text=example.question_text if question_first else example.context_text,\n",
        "                    text_pair=example.context_text if question_first else example.question_text,\n",
        "                    padding=kwargs[\"padding\"],\n",
        "                    truncation=\"only_second\" if question_first else \"only_first\",\n",
        "                    max_length=kwargs[\"max_seq_len\"],\n",
        "                    stride=kwargs[\"doc_stride\"],\n",
        "                    return_tensors=\"np\",\n",
        "                    return_token_type_ids=True,\n",
        "                    return_overflowing_tokens=True,\n",
        "                    return_offsets_mapping=True,\n",
        "                    return_special_tokens_mask=True,\n",
        "                )\n",
        "\n",
        "                # When the input is too long, it's converted in a batch of inputs with overflowing tokens\n",
        "                # and a stride of overlap between the inputs. If a batch of inputs is given, a special output\n",
        "                # \"overflow_to_sample_mapping\" indicate which member of the encoded batch belong to which original batch sample.\n",
        "                # Here we tokenize examples one-by-one so we don't need to use \"overflow_to_sample_mapping\".\n",
        "                # \"num_span\" is the number of output samples generated from the overflowing tokens.\n",
        "                num_spans = len(encoded_inputs[\"input_ids\"])\n",
        "\n",
        "                # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
        "                # We put 0 on the tokens from the context and 1 everywhere else (question and special tokens)\n",
        "                p_mask = np.asarray(\n",
        "                    [\n",
        "                        [tok != 1 if question_first else 0 for tok in encoded_inputs.sequence_ids(span_id)]\n",
        "                        for span_id in range(num_spans)\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                # keep the cls_token unmasked (some models use it to indicate unanswerable questions)\n",
        "                if self.tokenizer.cls_token_id is not None:\n",
        "                    cls_index = np.nonzero(encoded_inputs[\"input_ids\"] == self.tokenizer.cls_token_id)\n",
        "                    p_mask[cls_index] = 0\n",
        "\n",
        "                features = []\n",
        "                for span_idx in range(num_spans):\n",
        "                    features.append(\n",
        "                        SquadFeatures(\n",
        "                            input_ids=encoded_inputs[\"input_ids\"][span_idx],\n",
        "                            attention_mask=encoded_inputs[\"attention_mask\"][span_idx],\n",
        "                            token_type_ids=encoded_inputs[\"token_type_ids\"][span_idx],\n",
        "                            p_mask=p_mask[span_idx].tolist(),\n",
        "                            encoding=encoded_inputs[span_idx],\n",
        "                            # We don't use the rest of the values - and actually\n",
        "                            # for Fast tokenizer we could totally avoid using SquadFeatures and SquadExample\n",
        "                            cls_index=None,\n",
        "                            token_to_orig_map={},\n",
        "                            example_index=0,\n",
        "                            unique_id=0,\n",
        "                            paragraph_len=0,\n",
        "                            token_is_max_context=0,\n",
        "                            tokens=[],\n",
        "                            start_position=0,\n",
        "                            end_position=0,\n",
        "                            is_impossible=False,\n",
        "                            qas_id=None,\n",
        "                        )\n",
        "                    )\n",
        "                features_list.append(features)\n",
        "\n",
        "        all_answers = []\n",
        "        for features, example in zip(features_list, examples):\n",
        "            model_input_names = self.tokenizer.model_input_names\n",
        "            fw_args = {k: [feature.__dict__[k] for feature in features] for k in model_input_names}\n",
        "\n",
        "            # Manage tensor allocation on correct device\n",
        "            with self.device_placement():\n",
        "                if self.framework == \"tf\":\n",
        "                    fw_args = {k: tf.constant(v) for (k, v) in fw_args.items()}\n",
        "                    start, end = self.model(fw_args)[:2]\n",
        "                    start, end = start.numpy(), end.numpy()\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        # Retrieve the score for the context tokens only (removing question tokens)\n",
        "                        fw_args = {k: torch.tensor(v, device=self.device) for (k, v) in fw_args.items()}\n",
        "                        # On Windows, the default int type in numpy is np.int32 so we get some non-long tensors.\n",
        "                        fw_args = {k: v.long() if v.dtype == torch.int32 else v for (k, v) in fw_args.items()}\n",
        "                        start, end = self.model(**fw_args)[:2]\n",
        "                        start, end = start.cpu().numpy(), end.cpu().numpy()\n",
        "\n",
        "            min_null_score = 1000000  # large and positive\n",
        "            answers = []\n",
        "            for (feature, start_, end_) in zip(features, start, end):\n",
        "                # Ensure padded tokens & question tokens cannot belong to the set of candidate answers.\n",
        "                undesired_tokens = np.abs(np.array(feature.p_mask) - 1) & feature.attention_mask\n",
        "\n",
        "                # Generate mask\n",
        "                undesired_tokens_mask = undesired_tokens == 0.0\n",
        "\n",
        "                # Make sure non-context indexes in the tensor cannot contribute to the softmax\n",
        "                start_ = np.where(undesired_tokens_mask, -10000.0, start_)\n",
        "                end_ = np.where(undesired_tokens_mask, -10000.0, end_)\n",
        "\n",
        "                # Normalize logits and spans to retrieve the answer\n",
        "                start_ = np.exp(start_ - np.log(np.sum(np.exp(start_), axis=-1, keepdims=True)))\n",
        "                end_ = np.exp(end_ - np.log(np.sum(np.exp(end_), axis=-1, keepdims=True)))\n",
        "\n",
        "                if kwargs[\"handle_impossible_answer\"]:\n",
        "                    min_null_score = min(min_null_score, (start_[0] * end_[0]).item())\n",
        "\n",
        "                # Mask CLS\n",
        "                start_[0] = end_[0] = 0.0\n",
        "\n",
        "                starts, ends, scores = self.decode(\n",
        "                    start_, end_, kwargs[\"topk\"], kwargs[\"max_answer_len\"], undesired_tokens\n",
        "                )\n",
        "                if not self.tokenizer.is_fast:\n",
        "                    char_to_word = np.array(example.char_to_word_offset)\n",
        "\n",
        "                    # Convert the answer (tokens) back to the original text\n",
        "                    # Score: score from the model\n",
        "                    # Start: Index of the first character of the answer in the context string\n",
        "                    # End: Index of the character following the last character of the answer in the context string\n",
        "                    # Answer: Plain text of the answer\n",
        "                    answers += [\n",
        "                        {\n",
        "                            \"score\": score.item(),\n",
        "                            \"start\": np.where(char_to_word == feature.token_to_orig_map[s])[0][0].item(),\n",
        "                            \"end\": np.where(char_to_word == feature.token_to_orig_map[e])[0][-1].item(),\n",
        "                            \"answer\": \" \".join(\n",
        "                                example.doc_tokens[feature.token_to_orig_map[s] : feature.token_to_orig_map[e] + 1]\n",
        "                            ),\n",
        "                        }\n",
        "                        for s, e, score in zip(starts, ends, scores)\n",
        "                    ]\n",
        "                else:\n",
        "                    # Convert the answer (tokens) back to the original text\n",
        "                    # Score: score from the model\n",
        "                    # Start: Index of the first character of the answer in the context string\n",
        "                    # End: Index of the character following the last character of the answer in the context string\n",
        "                    # Answer: Plain text of the answer\n",
        "                    question_first = bool(self.tokenizer.padding_side == \"right\")\n",
        "                    enc = feature.encoding\n",
        "\n",
        "                    # Sometimes the max probability token is in the middle of a word so:\n",
        "                    # - we start by finding the right word containing the token with `token_to_word`\n",
        "                    # - then we convert this word in a character span with `word_to_chars`\n",
        "                    answers += [\n",
        "                        {\n",
        "                            \"score\": score.item(),\n",
        "                            \"start\": enc.word_to_chars(\n",
        "                                enc.token_to_word(s), sequence_index=1 if question_first else 0\n",
        "                            )[0],\n",
        "                            \"end\": enc.word_to_chars(enc.token_to_word(e), sequence_index=1 if question_first else 0)[\n",
        "                                1\n",
        "                            ],\n",
        "                            \"answer\": example.context_text[\n",
        "                                enc.word_to_chars(enc.token_to_word(s), sequence_index=1 if question_first else 0)[\n",
        "                                    0\n",
        "                                ] : enc.word_to_chars(enc.token_to_word(e), sequence_index=1 if question_first else 0)[\n",
        "                                    1\n",
        "                                ]\n",
        "                            ],\n",
        "                        }\n",
        "                        for s, e, score in zip(starts, ends, scores)\n",
        "                    ]\n",
        "\n",
        "            if kwargs[\"handle_impossible_answer\"]:\n",
        "                answers.append({\"score\": min_null_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n",
        "\n",
        "            answers = sorted(answers, key=lambda x: x[\"score\"], reverse=True)[: kwargs[\"topk\"]]\n",
        "            all_answers += answers\n",
        "\n",
        "        if len(all_answers) == 1:\n",
        "            return all_answers[0]\n",
        "        return all_answers\n",
        "\n",
        "\n",
        "    def decode(\n",
        "        self, start: np.ndarray, end: np.ndarray, topk: int, max_answer_len: int, undesired_tokens: np.ndarray\n",
        "    ) -> Tuple:\n",
        "        \"\"\"\n",
        "        Take the output of any :obj:`ModelForQuestionAnswering` and will generate probabilities for each span to be the\n",
        "        actual answer.\n",
        "\n",
        "        In addition, it filters out some unwanted/impossible cases like answer len being greater than max_answer_len or\n",
        "        answer end position being before the starting position. The method supports output the k-best answer through\n",
        "        the topk argument.\n",
        "\n",
        "        Args:\n",
        "            start (:obj:`np.ndarray`): Individual start probabilities for each token.\n",
        "            end (:obj:`np.ndarray`): Individual end probabilities for each token.\n",
        "            topk (:obj:`int`): Indicates how many possible answer span(s) to extract from the model output.\n",
        "            max_answer_len (:obj:`int`): Maximum size of the answer to extract from the model's output.\n",
        "            undesired_tokens (:obj:`np.ndarray`): Mask determining tokens that can be part of the answer\n",
        "        \"\"\"\n",
        "        # Ensure we have batch axis\n",
        "        if start.ndim == 1:\n",
        "            start = start[None]\n",
        "\n",
        "        if end.ndim == 1:\n",
        "            end = end[None]\n",
        "\n",
        "        # Compute the score of each tuple(start, end) to be the real answer\n",
        "        outer = np.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))\n",
        "\n",
        "        # Remove candidate with end < start and end - start > max_answer_len\n",
        "        candidates = np.tril(np.triu(outer), max_answer_len - 1)\n",
        "\n",
        "        #  Inspired by Chen & al. (https://github.com/facebookresearch/DrQA)\n",
        "        scores_flat = candidates.flatten()\n",
        "        if topk == 1:\n",
        "            idx_sort = [np.argmax(scores_flat)]\n",
        "        elif len(scores_flat) < topk:\n",
        "            idx_sort = np.argsort(-scores_flat)\n",
        "        else:\n",
        "            idx = np.argpartition(-scores_flat, topk)[0:topk]\n",
        "            idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
        "\n",
        "        starts, ends = np.unravel_index(idx_sort, candidates.shape)[1:]\n",
        "        desired_spans = np.isin(starts, undesired_tokens.nonzero()) & np.isin(ends, undesired_tokens.nonzero())\n",
        "        starts = starts[desired_spans]\n",
        "        ends = ends[desired_spans]\n",
        "        scores = candidates[0, starts, ends]\n",
        "\n",
        "        return starts, ends, scores\n",
        "\n",
        "\n",
        "    def span_to_answer(self, text: str, start: int, end: int) -> Dict[str, Union[str, int]]:\n",
        "        \"\"\"\n",
        "        When decoding from token probabilities, this method maps token indexes to actual word in the initial context.\n",
        "\n",
        "        Args:\n",
        "            text (:obj:`str`): The actual context to extract the answer from.\n",
        "            start (:obj:`int`): The answer starting token index.\n",
        "            end (:obj:`int`): The answer end token index.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary like :obj:`{'answer': str, 'start': int, 'end': int}`\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        token_idx = char_start_idx = char_end_idx = chars_idx = 0\n",
        "\n",
        "        for i, word in enumerate(text.split(\" \")):\n",
        "            token = self.tokenizer.tokenize(word)\n",
        "\n",
        "            # Append words if they are in the span\n",
        "            if start <= token_idx <= end:\n",
        "                if token_idx == start:\n",
        "                    char_start_idx = chars_idx\n",
        "\n",
        "                if token_idx == end:\n",
        "                    char_end_idx = chars_idx + len(word)\n",
        "\n",
        "                words += [word]\n",
        "\n",
        "            # Stop if we went over the end of the answer\n",
        "            if token_idx > end:\n",
        "                break\n",
        "\n",
        "            # Append the subtokenization length to the running index\n",
        "            token_idx += len(token)\n",
        "            chars_idx += len(word) + 1\n",
        "\n",
        "        # Join text with spaces\n",
        "        return {\n",
        "            \"answer\": \" \".join(words),\n",
        "            \"start\": max(0, char_start_idx),\n",
        "            \"end\": min(len(text), char_end_idx),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-89ff3acd1281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'is_tf_available' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRxgy4mhC3ab"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "051YKzwiCvWV",
        "outputId": "8228229e-149a-4e22-b87e-576682b6c93e"
      },
      "source": [
        "!pip install simpletransformers\n",
        "import logging\n",
        "\n",
        "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "train_data = [\n",
        "    {\n",
        "        \"context\": \"Mistborn is a series of epic fantasy novels written by American author Brandon Sanderson.\",\n",
        "        \"qas\": [\n",
        "            {\n",
        "                \"id\": \"00001\",\n",
        "                \"is_impossible\": False,\n",
        "                \"question\": \"Who is the author of the Mistborn series?\",\n",
        "                \"answers\": [\n",
        "                    {\n",
        "                        \"text\": \"Brandon Sanderson\",\n",
        "                        \"answer_start\": 71,\n",
        "                    }\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"The first series, published between 2006 and 2008, consists of The Final Empire,\"\n",
        "                   \"The Well of Ascension, and The Hero of Ages.\",\n",
        "        \"qas\": [\n",
        "            {\n",
        "                \"id\": \"00002\",\n",
        "                \"is_impossible\": False,\n",
        "                \"question\": \"When was the series published?\",\n",
        "                \"answers\": [\n",
        "                    {\n",
        "                        \"text\": \"between 2006 and 2008\",\n",
        "                        \"answer_start\": 28,\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"00003\",\n",
        "                \"is_impossible\": False,\n",
        "                \"question\": \"What are the three books in the series?\",\n",
        "                \"answers\": [\n",
        "                    {\n",
        "                        \"text\": \"The Final Empire, The Well of Ascension, and The Hero of Ages\",\n",
        "                        \"answer_start\": 63,\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"00004\",\n",
        "                \"is_impossible\": True,\n",
        "                \"question\": \"Who is the main character in the series?\",\n",
        "                \"answers\": [],\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "eval_data = [\n",
        "    {\n",
        "        \"context\": \"The series primarily takes place in a region called the Final Empire \"\n",
        "                   \"on a world called Scadrial, where the sun and sky are red, vegetation is brown, \"\n",
        "                   \"and the ground is constantly being covered under black volcanic ashfalls.\",\n",
        "        \"qas\": [\n",
        "            {\n",
        "                \"id\": \"00001\",\n",
        "                \"is_impossible\": False,\n",
        "                \"question\": \"Where does the series take place?\",\n",
        "                \"answers\": [\n",
        "                    {\n",
        "                        \"text\": \"region called the Final Empire\",\n",
        "                        \"answer_start\": 38,\n",
        "                    },\n",
        "                    {\n",
        "                        \"text\": \"world called Scadrial\",\n",
        "                        \"answer_start\": 74,\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"\\\"Mistings\\\" have only one of the many Allomantic powers, while \\\"Mistborns\\\" have all the powers.\",\n",
        "        \"qas\": [\n",
        "            {\n",
        "                \"id\": \"00002\",\n",
        "                \"is_impossible\": False,\n",
        "                \"question\": \"How many powers does a Misting possess?\",\n",
        "                \"answers\": [\n",
        "                    {\n",
        "                        \"text\": \"one\",\n",
        "                        \"answer_start\": 21,\n",
        "                    }\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"00003\",\n",
        "                \"is_impossible\": True,\n",
        "                \"question\": \"What are Allomantic powers?\",\n",
        "                \"answers\": [],\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "# Configure the model\n",
        "#impletransformers.language_modeling.LanguageModelingModel(self, model_type, model_name, args=None, use_cuda=True, cuda_device=-1, **kwargs,)\n",
        "model_args = QuestionAnsweringArgs()\n",
        "model_args.train_batch_size = 16\n",
        "model_args.evaluate_during_training = True\n",
        "\n",
        "model = QuestionAnsweringModel(\n",
        "    \"roberta\", \"roberta-base\", args=model_args, use_cuda=False, cuda_device=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.train_model(train_data, eval_data=eval_data)\n",
        "\n",
        "# Evaluate the model\n",
        "result, texts = model.eval_model(eval_data)\n",
        "\n",
        "# Make predictions with the model\n",
        "to_predict = [\n",
        "    {\n",
        "        \"context\": \"Vin is a Mistborn of great power and skill.\",\n",
        "        \"qas\": [\n",
        "            {\n",
        "                \"question\": \"What is Vin's speciality?\",\n",
        "                \"id\": \"0\",\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "answers, probabilities = model.predict(to_predict)\n",
        "--overwrite_output_dir\n",
        "print(answers)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.7/dist-packages (0.61.13)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.88.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.10.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.1.5)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.12.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.1.96)\n",
            "Requirement already satisfied: transformers>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.10.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.4)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.62.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.12.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (0.0.17)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (4.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers>=4.2.0->simpletransformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=4.2.0->simpletransformers) (2.4.7)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.24)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (5.0.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (5.4.8)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.1.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb>=0.10.32->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2021.8.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2.0.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.7.4.post0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.4)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (21.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.6.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.2.0->simpletransformers) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.2.0->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.1.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (5.1.1)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.5)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.0)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.18.2)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (6.4.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.5)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.0)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: ipython<8.0,>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (7.27.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.12.3)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.3)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.0.20)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (22.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.7.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.12.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.8.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-625d2582d032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/simpletransformers/question_answering/question_answering_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             raise ValueError(\n\u001b[1;32m    421\u001b[0m                 \u001b[0;34m\"Output directory ({}) already exists and is not empty.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m                 \u001b[0;34m\"Use --overwrite_output_dir to overcome.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             )\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Output directory (outputs/) already exists and is not empty.Use --overwrite_output_dir to overcome."
          ]
        }
      ]
    }
  ]
}